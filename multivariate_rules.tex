
\section{Quantile-based classifiers for multivariate data}
\label{sec:multivariate-classifier}

Having characterized the inherently univariate quantile classifier, our goal
then is to construct a classification method for data with multivariate features
based upon the quantile classifier.  We begin by extending some of the notation
from the previous section to the multivariate setting.

First, consider two populations $\Pi_0$ and $\Pi_1$ with corresponding
distribution functions $F_0$ and $F_1$ each on $\mathbb{R}^{p}$, and further let
$F_{0j}$ and $F_{1j}$ denote the marginal distribution functions for the $j$-th
variable with respect to $F_0$ and $F_1$, $j = 1, \dots, p$.  Then we denote the
quantile distance of a point $z \in \mathbb{R}$ to the $\theta$-th quantile of a
population's $j$-th component as
\begin{equation}
  \label{eq:multivariate-phi}
  \Phi_{ij}(z, \theta) =
  \rho_{\theta}\Big(z - F_{ij}^{-1}(\theta) \Big),
  \hspace{5mm} i = 0, 1, \hspace{3mm} j = 1, \dots, p.
\end{equation}
Then the difference of the quantile distances of a point $z$ to the populations'
$\theta$-th quantile of the $j$-th component is defined as
\begin{equation}
  \Lambda_j (z, \theta) = \Phi_{1j}(z, \theta) - \Phi_{0j}(z, \theta),
  \hspace{5mm} j = 1, \dots, p.
\end{equation}
The main idea of the composite quantile-based classifiers proposed in the next
section is to aggregate the discriminatory information contained in each of the
$\Lambda_j$'s to construct a classification method for data with multivariate
features.


\subsection{Composite quantile-based classifiers}
\label{sec:varying-coefficient}

In this section we propose the family of composite quantile-based classifiers.
We begin by motivating the the form of the classifiers.  Consider a point
$\vec{z} = (z_1, \dots, z_p) \in \mathbb{R}^p$ and quantile levels
$\vec{\theta} = (\theta_1, \dots, \theta_p) \in (0, 1)^p$.  If the difference of
the quantile distances of the $j$-th component $\Lambda_j(z_j, \theta_j)$ is
positive, this provides some descriminatory information indicating that $z_j$ is
closer to the $\theta_j$-th quantile of the $j$-th component of population
$\Pi_0$ than it is to the $\theta_j$-th quantile of the $j$-th component of
population $\Pi_1$, and conversely if $\Lambda_j(z_j, \theta_j)$ is negative.
Furthermore, differences of larger magnitudes provide stronger information than
those with smaller magnitudes.

The question of interest is how to construct a classification method based on
information obtained from each of the difference of the quantile distances
$\Lambda_j(z_j, \theta_j)$.  Intuitively, we want to classify an observation to
the population for which it has more components that are closer to the
population's component-wise quantiles.  A natural way to incorporate both the
distances and the number of components that an observation is closer to is to
simply sum up the $\Lambda_j(z_j, \theta_j)$'s and classify according to whether
the result is positive or negative.  This approach has a few difficulties
however.  One potential drawback is that equal weights are given to each of
features.  Naturally, some features may potentially be more discriminatory and
we would like to draw more heavily on these features for classification
purposes.  The second is an issue of scaling.  Features with a larger scale than
other features will implicitly be given higher discriminatory weight under this
scheme.  Finally, we would like to be able to account for dependencies that may
exist in the data.  With these issues in mind we propose using a linear
combination of the $\Lambda_j(z_j, \theta_j)$'s as follows.  We call this family
of classifiers \emph{composite quantile-based classifiers}.
\begin{equation}
  \label{eq:composite-classifier}
  \text{For an observation $\vec{z}$, classify to:} \hspace{3mm}
  \left\{
    \begin{array}{lll}
      \Pi_0, & & \alpha_0 + \sum_{j=1}^p \alpha_j \,
                 \Lambda_j (z_j, \theta_j) > 0 \\[1ex]
      \Pi_1, & & \text{otherwise}
    \end{array} ,
  \right.
\end{equation}
for $\vec{\alpha} = (\alpha_0, \dots, \alpha_p) \in \mathbb{R}^{p+1}$.  By
introducing the coefficients to the classification model this in principle
provides the ability to both handle the differences in scale of the features and
to provide varying weight to variables as a function of their discriminatory
power. Additionally, while we do not explicitly model dependencies between the
features, allowing the coefficients to take on both positive and negative values
permits the model to account for features' joint effects and to select a good
linear combination of features as the decision rule boundary.  Finally, the
coefficient $\alpha_0$ can be interpreted as an offset to classify an
observation when the linear combination of the differences of the quantile
distances is small.  This is useful when the prior probability of an observation
being drawn from one class is higher than for that the other.  When this is the
case, we can require a higher threshold of closeness for an observation to a
population that has a low prior probability before we decide to classify the
observation to that population.


\subsubsection{Properties of composite quantile-based classifiers}
\label{sec:aggregate-classifier-consistency}

Having presented the theoretical form of the composite quantile-based
classifiers, we now consider the consistency of the empirical version as well as
the form of the classifier decision rule boundary.  Firstly, let us denote
$\Psi(\vec{\theta})$ as the classification rate for composite quantile-based
classifiers given by
\begin{equation}
  \label{eq:multivariate-theoretical-rate}
  \Psi(\vec{\theta}) =
  \pi_0 \int \ind\bigg(
  \alpha_0 + \sum_{j=1}^p \alpha_j \, \Lambda_j (z_j, \theta_j) > 0
  \bigg)\, dP_0(\vec{z}) +
  \pi_1 \int \ind\bigg(
  \alpha_0 + \sum_{j=1}^p \alpha_j \, \Lambda_j (z_j, \theta_j) \leq 0
  \bigg)\, dP_1(\vec{z}).
\end{equation}
For the theorem below we will need the following assumptions.  Let $\delta$ be a
small positive constant.
\begin{enumerate}[label=\emph{Assumption \arabic*.}, align=left]
\item $F_{ij}^{-1}$ is a continuous function of
  $\theta,~ i=0,1,~ j=1, \dots, p$.
\item $\prob\Big(
  \alpha_0 + \sum_{j=1}^p \alpha_j\, \Lambda_j (Z_j, \theta_j) = 0
  \Big) = 0$ for all
  $\theta \in [\delta, 1 - \delta]$.
\item There is a unique $\tilde{\theta}_j$ that satisfies
  $\tilde{\theta}_j = \argmax_{\theta \in T} \Psi_j(\theta), j = 1, \dots, p$.
\end{enumerate}

\begin{theorem}
  \label{thm:multivariate-consistency}
  Denote
  $\tilde{\vec{\theta}} = \Big( \tilde{\theta}_1, \dots, \tilde{\theta}_P
  \Big)$.  Then under assumptions 1-3 it follows that
  $\Psi(\hat{\vec{\theta}}_n) \convp \Psi(\tilde{\vec{\theta}})$.
\end{theorem}
This result shows that the classification rate of the empirical version of
composite quantile-based classifiers converges to that of the composite
quantile-based classifier using the component-wise most discriminatory choices
of quantile level.

Next, we consider the decision rule form of composite quantile-based
classifiers.  Recall that the composite quantile-based family of classifiers is
defined as expression (\ref{eq:composite-classifier}).  From this expression we
see that the decision rule boundary is given by
\begin{equation}
  \label{eq:composite-classifiers-boundary}
  \alpha_0 + \sum_{j=1}^p \alpha_j\, \Lambda_j (z_j, \theta_j) = 0.
\end{equation}
As an aside, we note that the set of $\vec{z}$ that satisfy
(\ref{eq:composite-classifiers-boundary}) may be empty for certain choices of
$(\alpha_0, \dots, \alpha_p, \allowbreak\theta_1, \allowbreak\dots,
\allowbreak\theta_p)$, which corresponds to a decision rule that exclusively
classifies observations to one of the two classes.  If the solution set is
nonempty, then we can show that the decision rule boundary is a piecewise linear
function with a particularly simple form, as is discussed in what follows.  Let
us define the component-wise quantile classifier decision boundary $\tau_j$ to
be the unique point satisfying
$\Lambda_j (\tau_j, \theta_j) = 0,~ j = 1, \dots, p$.  We can assume without
loss of generality that $\tau_j$ is unique because it is only non-unique when
$\Lambda(\,\cdot\,,\, \theta_j)$ is identically zero, in which case the problem
is effectively reduced to $p - 1$ dimensions.  It follows then that
\begin{equation}
  \label{eq:lambda}
  \sign\Big(F_{1j}^{-1}(\theta_j) - F_{0j}^{-1}(\theta_j)\Big)~ \Lambda_j(z_j, \theta_j) =
  \left\{
    \begin{array}{lll}
      \tau_j - F_{(0)j}^{-1}(\theta_j),
      & \hspace{5mm}
      & z_j \leq F_{(0)j}^{-1}(\theta_j) \\[0.5ex]
      \tau_j - z_j,
      & \hspace{5mm}
      & F_{(0)j}^{-1}(\theta_j) < z_j < F_{(1)j}^{-1}(\theta_j) \\[0.5ex]
      \tau_j - F_{(1)j}^{-1} (\theta_j),
      & \hspace{5mm}
      & \text{otherwise} \\
    \end{array}.
  \right.
\end{equation}
The expression in (\ref{eq:lambda}) leads to the decision rule form of composite
quantile-based classifiers.  Conceptually, we can divide $\mathbb{R}^p$ into
$3^p$ hypercubes so that the decision rule boundary set is either empty or is an
affine set within each hypercube.


\begin{proposition}
  \label{lem:decision-rule-form}
  Define
  $\alpha_j^{*} = \alpha_j\, \sign\!\Big(F_1^{-1}(\theta_j) -
  F_0^{-1}(\theta_j)\Big)$, and let $L$, $M$, and $U$ (for lower, middle and
  upper) be sets that partition the set of indices $\{1, \dots, p\}$.  Define
  the hypercube $\mathcal{H}_{L,M,U}$ to be the set given by
  \begin{equation}
    \mathcal{H}_{L,M,U} = 
    \prod_{{\ell} \in L} \big( -\infty,~ F_{(0),{\ell}}^{-1}(\theta_{\ell}) \big)
    \prod_{m \in M} \big[ F_{(0),m}^{-1}(\theta_m),~ F_{(1),m}^{-1}(\theta_m) \big]
    \prod_{u \in U} \big( F_{(1),u}^{-1}(\theta_{u}),~ \infty \big),
  \end{equation}
  where in this context the products denote the usual Cartesian product.  Then
  the decision rule boundary for composite quantile-based classifiers on the
  domain $\mathcal{H}_{L,M,U}$ is the possibly empty set given by
  \begin{equation}
    \label{eq:composite-classifiers-general}
    \begin{split}
      \mathcal{D}_{L,M,U} = \left\{
        \vec{z} \in \mathcal{H}_{L,M,U}:
        \left[
          \alpha_0 + 
          \sum_{{\ell} \in L} \alpha_{\ell}^{*} \Big(\tau_{\ell} - F_{(0),{\ell}}^{-1}(\theta_{\ell})\Big) +
          \sum_{u \in U} \alpha_{u}^{*} \Big(\tau_{u} - F_{(1),u}^{-1}(\theta_{u})\Big)
        \right]
      \right. \\[1ex]
      \left.
        + \sum_{m \in M} \alpha_m^{*} (z_m - \tau_m) ~=~ 0
      \right\} .
    \end{split}
  \end{equation}
  Furthermore, the decision rule boundary is a continuous function of $\vec{z}$.
  
\end{proposition}




\subsection{Classifier model selection}
\label{sec:model-selection}

Having introduced a family of classifiers in Section
\ref{sec:varying-coefficient}, we are in need a method by which to select a
particular model based upon the data in hand.  This requires selection of both a
vector of component-wise quantile levels, and a vector of component
coefficients.  To perform model selection, we propose a two-step process as
described in Sections \ref{sec:choice-of-quantile-lev} and
\ref{sec:variable-coefficients}.




\subsubsection{Choice of quantile levels}
\label{sec:choice-of-quantile-lev}

As stated before, the fundamental motivation for the family of classifiers
proposed in this paper is the result that for univariate data and under some
assumptions, the decision rule based upon the distances of an observation to the
corresponding within-class quantiles for the optimal choice of quantile levels
is the Bayes rule.  Thus, we propose using these most powerful choices of
quantile level with respect to each feature's discriminatory ability, taken
one-at-a-time.  This statement is developed more precisely below.

To begin, let us suppose that we have $p$-dimensional observations
$\vec{x}_1, \dots, \vec{x}_n$, and we let $x_{ij}$ denote the $j$-th component
of $\vec{x}_i$; $i = 1, \dots, n,~ j = 1, \dots, p$.  Then we define the
empirical quantile for the $j$-th feature of the $i$-th population to be any
solution satisfying
\begin{equation}
  \label{eq:multivariate-empirical-quantile}
  \hat{F}_{ijn}^{-1} (\theta) = \argmin_q \left\{
    \theta \sum_{\substack{ \{ \ell:\, \vec{x}_{\ell} \in \Pi_i  \\[0.5mm] x_{\ell j} > q \} }}
    |x_{\ell j} - q| ~+~
    (1 - \theta)
    \sum_{\substack{ \{ \ell:\, \vec{x}_{\ell} \in \Pi_i  \\[0.5mm] x_{\ell j} \leq q \} }}
    |x_{\ell j} - q|
  \right\}.
\end{equation}
Having defined the $\theta$-th empirical quantile for the $j$-th feature of the
$i$-th population, it follows that the empirical difference of the quantile
distances of a point $z \in \mathbb{R}$ to the population's $\theta$-th
quantiles of the $j$-th feature is defined as
\[
  \Lambda_{jn} (z, \theta) =
  \rho_{\theta}\Big(z - \hat{F}_{1jn}^{-1}(\theta)\Big) -
  \rho_{\theta}\Big(z - \hat{F}_{0jn}^{-1}(\theta)\Big).
\]
Then we further define the observed rate of correct classification for the
$\theta$-th quantile of the $j$-th feature as
\begin{equation}
  \Psi_{jn}(\theta) = \frac{1}{n}
  \left[
    \sum_{x_i \in \Pi_0}
    \mathbbm{1}\Big( \Lambda_{jn}(x_{ij}, \theta) > 0 \Big) +
    \sum_{x_i \in \Pi_1}
    \mathbbm{1}\Big( \Lambda_{jn}(x_{ij}, \theta) \leq 0 \Big)
  \right].
\end{equation}
Finally, define the empirically optimal quantile classifier for the $j$-th
feature to be any solution to the equation
\begin{equation}
  \hat{\theta}_{jn} = \argmax_{\theta \in T} \Psi_{jn}(\theta),
\end{equation}
for $T = [ \delta, 1 - \delta]$ where $\delta$ is an arbitrarily small positive
constant.  Then we propose selecting $\hat{\theta}_{jn}$ as the choice of
quantile level, $j = 1, \dots, p$.




\subsubsection{Choice of linear combination coefficients}
\label{sec:variable-coefficients}

To motivate the choice of linear combination coefficients, we note the following
observation.  If we write
$z_j^{*} = \Lambda_j (z_j, \theta_j);~ j = 1, \dots, p$, then we can express the
classification rule in equation (\ref{eq:composite-classifier}) as follows:
\begin{equation}
  \label{eq: classifier transformed data}
  \text{For an observation $\vec{z}$, classify to:} \hspace{3mm} \left\{ 
    \begin{array}{lcl}
      \Pi_0, & & \alpha_0 + \sum_{j = 1}^p \alpha_j z_j^{*} ~>~ 0 \\[1ex]
      \Pi_1, & & \text{otherwise} \\[0ex]
    \end{array} .
  \right.
\end{equation}
From this form it is readily apparent that the the classification rule decision
boundary for the classifier in (\ref{eq: classifier transformed data}) is the
same as that of the decision rule boundary obtained from the logistic regression
model with $\vec{z}^{*} = (z_1^{*}, \dots, z_p^{*})$ as the predictor variables.
From this perspective, it is natural to select the classifier component
coefficients by using the logistic regression coefficient estimates obtained
from the transformed training data where the transformation is taken to be
$x_{ij}^{*} = \Lambda(x_{ij}, \theta_j)$ for all $i, j$.  Indeed, this is
exactly the approach that we propose, using a two step process as follows: first
choose quantile levels based on each feature's empirically optimal quantile
level in terms of class prediction, and then for these fixed choices of quantile
levels use penalized logistic regression on the transformed training data to
select a choice of linear combination coefficients.  A full presentation of the
two-step parameter selection process is provided in Section
\ref{sec:classifier-algorithm}.




\subsubsection{Connections to existing methods}
\label{sec:similarities-to-existing}

Composite quantile-based classifiers share a relationship with a number of
existing methods such as quantile-based classifiers, and more generally the
class of distance-based classifiers to which quantile-based classifiers belongs.
Distance-based classifiers are a type of classifier that assign an observation
to a population which it is deemed closer to by some measure of distance.  More
formally, let $\vec{\xi}_0$ and $\vec{\xi}_1$ be $p$-variate statistics
representing populations $\Pi_0$ and $\Pi_1$, respectively, and further let
$d(\cdot, \cdot)$ denote a chosen distance measure. Then, following the notation
used in \cite{hennig2016}, a distance-based classifier has the following form:
\begin{equation}
  \label{eq:distance-based-classifier}
  \text{For an observation $\vec{z}$, classify to:}  \hspace{3mm}
  \left\{
    \begin{array}{lll}
      \Pi_0, & & \sum_{j=1}^p
                 \Big\{
                 d(z_j, \xi_{1j}) - d(z_j, \xi_{0j})
                 \Big\} > 0 \\[1ex]
      \Pi_1, & & \text{otherwise}
    \end{array}.
  \right.
\end{equation}
Some examples of distance-based classifiers include nearest centroid
classification (e.g.\ \cite{hastie2009}), shrunken centroids
(\cite{tibshirani2002}, \cite{wang2007}), median-based classifiers
(\cite{jornsten2004}, \cite{ghosh2005}, \cite{hall2012}), and quantile-based
classifiers (\cite{hennig2016}).  When we restrict the linear combination
coefficients so that $\alpha_0 = 0$ and $\alpha_1 = \dots = \alpha_p > 0$, and
also require that $\theta_1 = \dots = \theta_p = \theta$, then the decision rule
is based upon
\begin{equation}
  \label{eq:boundary-equivalence}
  \alpha_0 + \sum_{j=1}^p \alpha_j \, \Lambda_j (z_j, \theta_j) ~>~ 0
  \hspace{5mm} \Longleftrightarrow \hspace{5mm}
  \sum_{j=1}^p \Lambda_j (z_j, \theta) > 0,
\end{equation}
which is the form of quantile-based classifiers.  Quantile-based classifiers
belong to the family of distance based classifiers, which can be seen by letting
$\vec{\xi}_i = \Big(F_{i1}^{-1}(\theta), \dots, F_{ip}^{-1}(\theta) \Big),~ i =
0, 1$, and further letting
$d_\theta(z_j, \xi_{ij}) = \rho_{\theta}(z_j - \xi_{ij})$, in which case
\begin{equation}
  \label{eq:quantile-is-distance-subset}
  \sum_{j=1}^p \Lambda_j (z_j, \theta) = \sum_{j=1}^p \Big\{ d_{\theta}(z_j,
  \xi_{1j}) - d_{\theta}(z_j, \xi_{0j}) \Big\}.
\end{equation}
So from this we see that composite quantile-based classifiers cover
quantile-based classifiers.  Furthermore, although composite quantile-based
classifiers are not in general distance based classifiers, we see that a subset
of the family of classifiers belongs to the set of distance based classifiers.

Another interesting connection to note is that the proposed choice of quantile
levels and variable coefficients from Sections \ref{sec:choice-of-quantile-lev}
and \ref{sec:variable-coefficients} selects a model from the composite
quantile-based classifiers that bears a number of similarities to the FANS
classifier proposed in \cite{fan2016}.  The FANS classifier starts from the idea
that for data with an equal prior probability of being a member from one of two
classes with corresponding class conditional densities $f_0$ and $f_1$, the
Bayes rule decision boundary is given by
$\left\{ \vec{z}{:}~ \log \frac{ f_0(\vec{z}) }{ f_1(\vec{z}) } = 0 \right\}$.
Then, if it is the case that conditional on class membership the features are
mutually independent, the Bayes rule decision boundary becomes
\begin{equation}
  \label{eq:bayes-rule-independent}
  \mathcal{D} = \left\{
    \vec{z} :~
    \log \frac{ f_{01}(z_1) }{ f_{11}(z_1) } +
    \cdots +
    \log \frac{ f_{0p}(z_p) }{ f_{1p}(z_p) }
    ~=~ 0
  \right\} ,
\end{equation}
where $f_{ij}$ corresponds to the class conditional marginal density for the
$j$-th feature of the $i$-th class.  The FANS classifier then proposes the
following generalization of the decision boundary:
\begin{equation}
  \label{eq:fans-rule}
  \mathcal{D}_{\scriptscriptstyle \text{FANS}} = \left\{
    \vec{z} :~ \alpha_0 + 
    \alpha_1 \log \frac{ f_{01}(z_1) }{ f_{11}(z_1) } +
    \cdots +
    \alpha_p \log \frac{ f_{0p}(z_p) }{ f_{1p}(z_p) }
    ~=~ 0
  \right\}.
\end{equation}
The classifier then estimates class conditional marginal densities $f_{ij}$
using nonparametric kernel density estimation, and selects the choice of
coefficients $\alpha_0, \dots, \alpha_p$ by using the penalized logistic
regression coefficient estimates obtained from the transformed training data
where the transformation is taken to be
$x_{ij}^{*} = \log \frac{ \hat{f}_{0j}(x_{ij}) }{ \hat{f}_{1j}(x_{ij}) }$ for
all $i, j$.

Having described the FANS method in brief, it is evident that although the
composite quantile-based classifiers and FANS classifiers are each motivated
from different perspectives, they share a number of common traits.  Both methods
start with component-wise transformations that each yield the Bayes rule
decision boundary in the univariate setting, and then perform logistic
regression on those transformed features to create a decision rule boundary
nonlinear in the original features.  The difference in the classifiers comes of
course from the transformation performed on each of the components.

One final point that was noted in \cite{fan2016} and bears repeating here, is
that instead of using penalized linear regression, composite quantile-based
classifiers may well use the SVM (linear kernel) or any other linear classifier
as a means through which to obtain the transformed feature coefficient values.
In this paper, as in \cite{fan2016}, we focus on using penalized linear
regression with the $\ell_1$ penalty.




\subsection{Algorithm for composite quantile-based classifiers}
\label{sec:classifier-algorithm}

In this section, we present an algorithm used to select a composite
quantile-based classifier model.  This algorithm is similar to the algorithm
proposed in \cite{fan2016}.

We begin with some observations that motivate the form of the algorithm.  One
issue that needs some consideration is the fact that the proposed choices of
quantile levels and linear combination coefficients are based upon a two-step
process.  Recall from Sections \ref{sec:choice-of-quantile-lev} and
\ref{sec:variable-coefficients} that the quantile levels are chosen first, with
each quantile level based on the optimal level with respect to the feature's
ability to predict class membership.  Then for the second step, the linear
combination coefficients are chosen based upon the transformed data where the
transformation is performed with respect to the choices of quantile levels from
the first step.  Because of this two-step process, the second step is
particularly prone to overfitting the model to the training data.  To mitigate
this undesirable behavior, the data set is split into two parts: one part to
train for the choice of quantile levels and corresponding within-class
quantiles, and the other part to train for the choice of penalty parameter used
to select the linear combination coefficients.  The question then is how to
split the data.  Based on empirical evidence, we suggest randomly splitting the
data into evenly-sized partitions.  In our experience, this equitable
partitioning tends to work well across different sizes and types of data.

While alleviating one problem, splitting the data leads to another concern:
namely that different splits of the data can lead to different model choices.
To protect against arbitrary assignments of the data, the data are split not
just once but multiple times, and a classification model is procured for each
split.  The final model is then obtained by averaging the individual models in
the following sense.  Let $f_i$ be one of the individual models among a total of
$L$ data splits.  Then a new observation $\vec{z}$ is classified according to
$\frac{1}{L} \sum_{i=1}^L f_i(\vec{z})$ where the decision rule boundary is 0
(furthermore, since the boundary rule is 0 we need not divide by $L$).  We note
that the process described here is similar in concept to the ideas presented in
bagging \cite{breiman1996} and random forests \cite{breiman2001}.  The composite
quantile-based classifiers algorithm is presented in its entirety as Algorithm
\ref{alg:classifier}.


\begin{algorithm}[ht]
  \caption{Composite quantile-based classifiers model selection algorithm}
  \label{alg:classifier}
  \DontPrintSemicolon
  \BlankLine
  \SetKwInOut{Input}{Data}
  \Input{$n$ pairs of observations
    $D = \left\{ (\vec{x}_i, y_i),~ i = 1, \dots, n \right\}$}

  \For{$\ell$ in 1 to $L$}{
    
    randomly split the data into two parts:
    $D_{\ell} = (D_{\ell 1}, D_{\ell 2})$

    \For{$j$ in $1$ to $p$}{

      $\hat{\theta}_{jn_{
          \raisebox{-0.5mm}{$\scriptscriptstyle \ell 1$}
        }} \leftarrow \argmax_{\theta \in T} \Psi_{jn_{
          \raisebox{-0.5mm}{$\scriptscriptstyle \ell 1$}
        }} (\theta)$
      by using Algorithm \ref{alg:empirically-optimal-classifier} with the
      training data in $D_{\ell 1}$ \;

      % using Algorithm \ref{alg:empirically-optimal-classifier}, calculate an
      % empirically optimal quantile level $\hat{\theta}_j$ for the $j$-th feature
      % in terms of the feature's ability to predict the class of the training
      % data in $D_{\ell 1}$ absent other features \;

      \For{$x_{ij}$\! in $D_{\ell 2}$}{

        $x_{ij}^{*} \leftarrow \Lambda_{jn_{
            \raisebox{-0.5mm}{$\scriptscriptstyle \ell 1$}
          }} \!\left(x_{ij}, \, \hat{\theta}_{jn_{
            \raisebox{-0.5mm}{$\scriptscriptstyle \ell 1$}
          }} \right)$

      }

    }

    $f_{\ell}(\cdot) \leftarrow$ the model obtained by fitting a penalized
    logistic regression model to the transformed data
    $\{(\vec{x}_i^{*}, y_i),~ i \in D_{\ell 2} \}$ and using cross validation to
    select the penalty parameter
    
  }

  $f \leftarrow \frac{1}{L} \sum_{\ell=1}^L f_{\ell}$ is the composite
  quantile-based classifier model

\end{algorithm}

% % old version
% \begin{proposition}
%   \label{thm:cqc-runtime}
%   Let $T$ be the number of penalty levels and $K$ be the number of folds
%   considered when performing penalized logistic regression with $k$-fold
%   cross-validation to select the linear combination coefficients in Step 9 of
%   Algorithm \ref{alg:classifier}.  Then the model selection algorithm for
%   composite quantile-based classifiers runs in
%   $\mathcal{O}\Big( Lnp \big[ n + KT \big]\Big)$ time.
% \end{proposition}

Two expressions for the run time complexity of Algorithm \ref{alg:classifier}
are presented in Proposition \ref{thm:cqc-runtime}.  The first expression
describes the complexity of a sequential implementation of the algorithm, while
the second expression considers an implementation utilizing parallel computing.
The opportunities for parallelism that are considered are the following.  At the
top level, the sub-models $f_1, \dots, f_L$ can each be obtained independently
of each other.  Furthermore, within the calculation of each sub-model with index
$\ell$, the feature-specific calculations can each be performed independently of
each other.  Thus, for $j$ in $1, \dots p$, both the calculation of the optimal
quantile level for the $j$-th feature with data $D_{\ell 1}$ and the
transformation of the $j$-th component of every $\vec{x}_i$ in data $D_{\ell 2}$
can be calculated independently of the other features (lines 3-8 of Algorithm
\ref{alg:classifier}).  Additionally, selection of the choice of linear
combination coefficients can be parallelized over the folds in the
cross-validation procedure (line 9 of Algorithm \ref{alg:classifier}).
Exploiting these opportunities for parallelism leads to the run time complexity
described below.

\begin{proposition}
  \label{thm:cqc-runtime}
  Let $T$ be the number of penalty levels and $K$ be the number of folds
  considered when performing penalized logistic regression with $k$-fold
  cross-validation to select the linear combination coefficients
  $\alpha_0, \dots, \alpha_p$.  Then the model selection algorithm for composite
  quantile-based classifiers runs in
  $\mathcal{O}\Big( Lnp \big[ n + KT \big]\Big)$ time.  Furthermore, if the
  number of compute nodes in a computing cluster topology is at least $L$, and
  $C$ denotes the smallest number of available cores from among the compute
  nodes, then the run time of Algorithm \ref{alg:classifier} is reduced to
  $\mathcal{O}\Big( \frac{np}{C} \big[ n + KT \big] \Big)$.
\end{proposition}

One question that arises is how to choose $L$, $K$, and $T$.  The number of data
splits (i.e. the size of $L$) suggested by \cite{fan2016} is 20, and we have
also found 20 splits to be sufficient to provide stable results.  Typical
choices for the number of folds and the size of the penalty parameter grid
(i.e. $K$ and $T$) for penalized logistic regression are often around 10 and
100, respectively.




\subsection{Classifier examples}
\label{sec:classifier-examples}

Having described the general form of the composite quantile-based classifiers,
we now consider a few illustrative examples of the classifiers in two
dimensions.  In each of the examples we use the optimal choice of quantile level
for each of the features to discriminate between the classes.  These are the
quantile levels that are chosen by the composite quantile-based classifiers in
the limit as the sample size goes to infinity, as a consequence of Lemma
\ref{lem:univariate-consistency}.  We also set the linear combination
coefficients as $(\alpha_0, \alpha_1, \alpha_2) = (0, 1, 1)$ and the prior
probability for each class as 1/2.




\subsubsection*{Example 1:  Two dimensional Gaussian data}
\label{sec:classifier-examples-gaussian}

In the first example, shown in Figure \ref{fig:gaussian-classify}, we consider
the classical setting of two Gaussian distributions.  The first class, shown in
yellow, follows a $N \big( (0, 0),\, \vec{I} \big)$ distribution, while the
second class, shown in light red, follows a $N \big( (2, 1),\, \vec{I} \big)$
distribution.  In the left panel, the inner circles for each class represent a
contour line for 68\% of the mass of the distribution, while the outer circles
represent a contour line for 95\% of the mass of the distribution.  Additionally
the decision rule boundary lines are shown for both the composite quantile-based
classifiers method and the Bayes rule.  The composite quantile-based classifiers
rule has a classification rate of 0.82, while the Bayes rule classification rate
is 0.87.

The same contour lines are shown on the right panel, but the figure is zoomed in
on the upper-right quadrant of the coordinate system in order to focus on the
decision rule boundary in more detail.  To begin with, we note that the marginal
distributions' Bayes rules have decision rule boundaries with values of $1$ and
$1/2$ for component 1 (the horizontal axis) and component 2 (the vertical axis),
respectively, which corresponds to the median quantile for each component (i.e.
$\theta_1 = \theta_2 = 0.5$).  It follows that the corresponding population
quantiles are given by $F_{01}^{-1}(\theta_1) = 0$, $F_{11}^{-1}(\theta_1) = 2$,
$F_{02}^{-1}(\theta_2) = 0$, and $F_{11}^{-1}(\theta_2) = 1$, and furthermore
that $\Lambda_1(1, \theta_1) = 0$ and $\Lambda_2(0.5, \theta_2) = 0$.  The slope
of the line within the box bounded by the within-class quantiles is $-1$.  Once
the second component (on the vertical axis) is either no less than $1$ or no
greater than $0$ then $\Lambda_2$ becomes a fixed value and the second component
becomes a free variable, while the first component is fixed at $0.5$ or $1.5$,
respectively.

\begin{figure}[ht]
  \caption{Composite quantile-based classifier for Gaussian distributions setting.}
  \label{fig:gaussian-classify}
  \centering
  \vspace{5mm}

  \begin{minipage}[t]{0.55\linewidth}
    \flushleft
    \includegraphics[width=0.96\textwidth]{gauss_ci}
  \end{minipage}
  \begin{minipage}[t]{0.44\linewidth}
    \flushright
    \includegraphics[width=1\textwidth]{gauss_quant_rule}
  \end{minipage}
\end{figure}




\subsubsection*{Example 2:  Two dimensional beta and gamma distributed data}
\label{sec:beta-gamma-example}

In the second example, shown in Figure \ref{fig:beta-and-gamma}, we consider
settings where the shape of the component-wise within-class marginal
distributions varies across populations.  In the left panel, we consider
populations where the within-class distributions follow independent beta
distributions, and in the right panel we consider populations where the
within-class distributions follow independent gamma distributions.  The
parameters of the beta and gamma distributions were chosen at random.  As in the
previous example, contour lines denoting 68\% and 95\% of the distributional
mass for each population are shown.  The Bayes rule decision boundary is shown
as the black line and the composite quantile-based decision rule boundary is
shown as the blue line.  The composite quantile-based classifiers rule has a
classification rate of 0.84 compared to 0.87 for the Bayes rule for the beta
distributed data, and a classification rate of 0.75 compared to 0.76 for the
gamma distributed data.

In both examples we see that the Bayes rule decision boundary is nonlinear, so
we can only at best hope to approximate it.  Fortunately, in both examples the
composite quantile-based decision rule boundary follows the Bayes rule decision
boundary well in the higher-density regions of the distributions.  As the Bayes
rule decision boundary moves into low-density distributional regions the
approximation is less accurate due to the form of the composite quantile-based
classifiers.  In our experience this is typical of composite quantile-based
classifiers: the approximation of the Bayes rule is often good at the center of
the data and is less accurate in the tails.

\begin{figure}[ht]
  \caption{Composite quantile-based classifier for beta- and gamma-distributed
    data settings.}
  \label{fig:beta-and-gamma}
  \centering
  \vspace{5mm}

  \begin{minipage}[t]{0.49\linewidth} \vspace{0mm}
    \centering
    Beta distributed data \\[2ex]
    \includegraphics[width=0.975\textwidth]{beta_cqc}
  \end{minipage}
  \begin{minipage}[t]{0.49\linewidth} \vspace{0mm}
    \centering
    Gamma distributed data \\[2ex]
    \includegraphics[width=0.975\textwidth]{gamma_cqc}
  \end{minipage}
\end{figure}

One limitation of composite quantile-based classifiers that is evident from
these examples is that the classification rate does not in general achieve the
Bayes rule classification rate even in the limit.  This is a consequence of the
piecewise linear form of the decision rule boundary.  We view this limitation as
a trade-off between simplicity of the classifier and its ability to perform well
in all situations.  A similar property holds for tree-based methods, which
perform a vertical or horizontal cut at each stage.  Because of the simple
nature of the classifier it is well-suited for high-dimensional problems, even
in the presence of limited data.  For settings where the Bayes rule decision
boundary is linear as in Example 1, a variant of composite quantile-based
classifiers is discussed in the following section that can achieve the Bayes
rule classification rate.


\subsection{Augmented composite quantile-based classifiers}
\label{sec:augmented}

In addition to the composite quantile-based classifiers proposed in this paper,
we propose a variant of these classifiers called augmented composite
quantile-based classifiers.  In this variant, the transformed features upon
which the decision rule boundary are constructed are augmented by the original
features.  In other words, the decision rule for an observation $\vec{z}$ is
based upon
\begin{equation}
  \label{eq:augmented-rule}
  % \alpha_0 + \sum_{j=1}^p \alpha_j \,\Lambda_j (z_j, \theta_j) + \sum_{j=1}^p \alpha_{2j} z_j
  \alpha_0 +
  \alpha_1 \,\Lambda_1 (z_1, \theta_1) +
  \dots +
  \alpha_p \,\Lambda_p (z_p, \theta_p) +
  \alpha_{p + 1} z_1 +
  \dots +
  \alpha_{2p} z_p.
\end{equation}
There are two main motivations for considering this variant.  One reason is that
when the decision rule boundary is approximately linear in the original
features, then augmenting the transformed data can result in a better
classification rate.  A second motivation for augmented composite quantile-based
classifiers is an issue that can arise due to the component-wise form of
classifiers.  If the marginal densities for a given feature are the same for the
two classes then the feature does not provide any contribution to the
classifier.  However, this deletion can result in loss of efficiency because
features that have no discriminative ability on their own may still contain
useful information when considered jointly.  Including the original features
ensures that component-wise deletion does not occur.

There is a trade-off that occurs when including the original features
in the model.  In some settings the original features can provide information to
the classifier helping to boost performance, but on the other hand augmenting
the data doubles the feature space and can add noise to the model.




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "cqc_paper"
%%% End:
