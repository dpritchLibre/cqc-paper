
\section{QUANTILE CLASSIFIERS FOR UNIVARIATE DATA}
\label{sec:univariate-classifier}

As noted in Section \ref{sec:intro}, the fundamental motivation for the family
of classifiers proposed in this paper is the result that for univariate data and
the optimal choice of quantile level, the decision rule based upon the distances
of an observation to the corresponding within-class quantiles is the Bayes rule.
The proposed classifiers for multivariate data are then constructed by
considering each component of the observed data as a 1-dimensional problem, and
then aggregating these most powerful univariate classifiers in some manner to
create a multivariate classifier.  From this perspective, we find it natural to
present our methodology by first considering the application of quantile
classifiers in the univariate setting.

The quantile classifier and the empirically optimal quantile classifier was
introduced in Hennig and Viroli (2016).  For completeness, the notation and
definitions are reintroduced in Sections \ref{sec:quantile-classifier} and
\ref{sec:empirical-classifier}.  In Section
\ref{sec:quantile-classifier-optimality} we present some results for the
quantile classifier in the univariate setting.  A closed-form expression for the
decision boundary for the quantile classifier for a fixed choice of quantile
level is derived, as well as a closed-form solution for the empirical quantile
for a given quantile level.  It is shown that these two results in conjunction
yield a decision rule that can be found in $\bigO(n)$ time, where $n$ is the
number of observations in the training data.  Consistency of both the estimated
optimal quantile level and that of the empirically optimal quantile classifier
is established.  Furthermore, an algorithm is proposed that obtains the decision
rule for the empirically optimal quantile classifier in $\bigO(n \log n)$ time.




\subsection{THE QUANTILE CLASSIFIER}
\label{sec:quantile-classifier}

Consider the quantile loss (also called the check loss) function defined as
\begin{align}
  \label{eq:check-loss}
  \begin{split}
    \rho_\theta(u)
    &= \ind(u > 0)\, \theta\, u  + \ind (u \leq 0)\, (1 - \theta)\, (-u)  \\[0ex]
    &= u\, \big[ \theta - \ind(u \leq 0) \big]
  \end{split}
\end{align}
\\[-1ex]
for some choice of $\theta \in (0,1)$.  This loss function has the important
property that for the $\theta$-th quantile level of a continuous univariate
random variable $X$ it induces the $\theta$-th quantile as the minimizer of the
expected distance to $X$.  That is,
\begin{equation}
  \label{eq:check-loss-min}
  \argmin_q \ev \big[ \rho_\theta (X - q) \big] = F_X^{-1}(\theta)
\end{equation}
for $F_X$ the distribution function of $X$.  Next, consider two populations
$\Pi_0$ and $\Pi_1$ with corresponding distribution functions $F_0$ and $F_1$ on
$\mathbb{R}$.  We define the quantile distance of a point $z$ to the
population's $\theta$-th quantile as
\begin{equation}
  \label{eq:phi}
  \Phi_i(z, \theta) = \rho_{\theta}\Big(z - F_i^{-1}(\theta)\Big),
  \hspace{5mm} i = 0, 1.
\end{equation}
Then the difference of the quantile distances of a point $z$ to the populations'
$\theta$-th quantiles is defined as
\begin{equation}
  \Lambda\,(z, \theta) = \Phi_1(z, \theta) - \Phi_0(z, \theta).
\end{equation}
With these definitions in hand, the $\theta$-th quantile-based classifier is
defined as follows.
\begin{equation}
  \label{eq:quantile-classifier}
  \text{For an observation $z$, classify to:} \hspace{5mm} \left\{ 
    \begin{array}{lcl}
      \Pi_0, & & \text{if} \hspace{3mm}\Lambda\,(z, \theta) > 0 \\[0ex]
      \Pi_1, & & \text{otherwise}
    \end{array}
  \right.
\end{equation}
Finally, let $Z$ be a random variable with a prior probability $\pi_0$ of being
a member of population $\Pi_0$, and $\pi_1 = 1 - \pi_0$ the prior probability of
being a member of population $\Pi_1$.  Then the probability of correctly
classifying an observed realization of $Z$ by the $\theta$-th quantile
classifier is given by
\begin{equation}
  \label{eq:theoretical-rate}
  \Psi(\theta) =
  \pi_0 \int \ind\Big( \Lambda(z, \theta) > 0 \Big)\, dP_0(z) +
  \pi_1 \int \ind\Big(\Lambda(z, \theta) \leq 0 \Big)\, dP_1(z).
\end{equation}
The expression given in (\ref{eq:theoretical-rate}) has the appealing property
that for the optimal choice of quantile level $\theta$ the classification rate
is equal to that of the Bayes classifier.  This result is formally stated as
Theorem \ref{thm:quantile-classifier-is-bayes}.

% \begin{theorem}
%   \label{thm:quantile-classifier-is-bayes}
%   Consider two populations $\Pi_0$ and $\Pi_1$ with corresponding density
%   functions $f_0$ and $f_1$ such that $f_0$ and $f_1$ are nonzero on the same
%   domain.  Let $Z$ be a random variable with a prior probability $\pi_0$ of
%   being a member of population $\Pi_0$, and $\pi_1 = 1 - \pi_0$ the prior
%   probability of being a member of population $\Pi_1$.  Further assume that
%   there is a point $z^{*}$ with $\pi_0\, f_0(z^{*}) = \pi_1\, f_1(z^{*})$ so
%   that $\pi_0\, f_0(z) < \pi_1\, f_1(z)$ for $z$ on one size of $z^{*}$, and
%   $\pi_0\, f_0(z) > \pi_1\, f_1(z)$ on the other side of $z^{*}$.  Then the
%   quantile classifier for an observed realization of $Z$ using the optimal
%   choice of quantile level achieves the Bayes rule classification probability.
% \end{theorem}

% \begin{proof}
%   This result and proof were given as Lemma 2 in \cite{hennig2016}.
% \end{proof}


% Next, let $x_1, \dots, x_n$ be samples from a population with a probability
% density on $\mathbb{R}$.  Then we define the $\theta^{\text{th}}$ empirical
% quantile for the population to be given by minimizer of the sample version of
% equation (\ref{eq: check loss min}) defined by:

% Next, let $\Pi_0$ and $\Pi_1$ be
% two populations with probability densities $P_0$ and $P_1$ on $\mathbb{R}$.
% Consider observations $x_{0, 1}, \dots, x_{0, n_0}$ sampled from population
% $\Pi_0$ and observations $x_{1, 1}, \dots, x_{1, n_1}$ sampled from population $\Pi_1$.
% Then we define $\widehat{F}^{-1}_i (\theta)$ to be the $\theta^{\text{th}}$
% empirical quantile for population $\Pi_i$, where
% \begin{equation}
%   \label{eq: empirical quantile}
%   \widehat{F}^{-1} (\theta) = \argmin_q \left\{ \theta \sum_{ x_{i} > q } |x_{i} - q| ~+~
%     (1 - \theta) \sum_{ x_{i} \leq q } |x_{i} - q| \right\}.
% \end{equation}
% Thus, the empirical quantile is the the minimizer of the sample version of
% equation (\ref{eq: check loss min}).  Finally, let $z$ be a new observation.
% Then classify $z$ to
% \begin{equation}
%   \label{eq: distance classifier}
%   \left\{ 
%     \begin{array}{lcl}
%       \Pi_0, & & \text{if} \hspace{3mm} \rho_{\theta} \left( z - \widehat{F}^{-1}_1 (\theta) \right) ~-~
%                  \rho_{\theta} \left( z - \widehat{F}^{-1}_0 (\theta) \right) ~>~ 0 \\[0ex]
%       \Pi_1, & & \text{otherwise}
%     \end{array}
%   \right.
% \end{equation}
% Conceptually, classify the observation to the population for which it has the
% smaller quantile distance to from the $\theta^{\text{th}}$ quantile.

% To use the classification rule given in equation (\ref{eq: distance classifier})
% in practice, it remains to select a choice of quantile level $\theta$.  In the
% following sections we consider two approaches for selecting the quantile level,
% with an eye towards the extension of the classification rule to the multivariate
% setting.


\subsection{THE EMPIRICALLY OPTIMAL QUANTILE CLASSIFIER}
\label{sec:empirical-classifier}

Let $x_1, \dots, x_m$ be samples from a population with a probability density on
$\mathbb{R}$.  Then we define the $\theta$-th empirical quantile for the
population to be given by any minimizer of the sample version of equation
(\ref{eq:check-loss-min}), defined by
\begin{equation}
  \label{eq:empirical-quantile}
  \hat{F}^{-1} (\theta) = \argmin_q \left\{
    \theta \sum_{ x_{i} > q } |x_{i} - q| ~+~
    (1 - \theta) \sum_{ x_{i} \leq q } |x_{i} - q|
  \right\}.
\end{equation}
It is interesting to note that the right-hand side of the above equation is
mentioned in \cite{koenker1978} in the context of quantile regression.  In
fact, we can view equation (\ref{eq:empirical-quantile}) as the solution to a
quantile regression problem, where the $x_i$'s correspond to the response
variables, and $q$ is a single regression coefficient corresponding to predictor
data with only an intercept term and no other covariates.  In light of this
fact, we may utilize any of the theory developed in the quantile regression
literature to solve this minimization problem.  However, it turns out that there
is a simple closed-form solution available for this particular special case of
quantile regression, which is presented as Lemma \ref{lem:empirical-quantlev}.

Having defined the $\theta$-th empirical quantile for the population, it follows
that the empirical difference of the quantile distances of a point $z$ to the
population's $\theta$-th quantiles is defined as
\[
  \Lambda_n (z, \theta) = \rho_{\theta}\Big(z - \hat{F}_{1n}^{-1}(\theta)\Big) -
  \rho_{\theta}\Big(z - \hat{F}_{0n}^{-1}(\theta)\Big).
\]
where $\hat{F}_{1n}$ is the estimate of the $\theta$-th quantile level of
population $\Pi_1$ based on the samples in $x_1, \dots, x_n$ that came from
population $\Pi_1$, and similarly for $\hat{F}_{0n}$.  Let $n_0$ and $n_1$
be the number of observations from $\Pi_0$, and $\Pi_1$, respectively.  Then we
further define the observed rate of correct classification for the $\theta$-th
quantile as
\begin{align}
  \begin{split}
    \Psi_n(\theta)
    &= \frac{n_0}{n_0 + n_1} \left[
      \frac{1}{n_0} \sum_{x_i \in \Pi_0}
      \mathbbm{1}\Big( \Lambda_n(x_i, \theta) > 0 \Big)
    \right] +
      \frac{n_1}{n_0 + n_1} \left[
        \frac{1}{n_1} \sum_{x_i \in \Pi_1}
        \mathbbm{1}\Big( \Lambda_n(x_i, \theta) \leq 0 \Big)
      \right] \\[1ex]
    &= \frac{1}{n} \left[
      \sum_{x_i \in \Pi_0} \mathbbm{1}\Big( \Lambda_n(x_i, \theta) > 0 \Big) +
      \sum_{x_i \in \Pi_1} \mathbbm{1}\Big( \Lambda_n(x_i, \theta) \leq 0 \Big)
    \right].
  \end{split}
\end{align}
Finally, define the empirically optimal quantile classifier to be any solution
to the equation
\begin{equation}
  \hat{\theta}_n = \argmax_{\theta \in T} \Psi_n(\theta)
\end{equation}
for $T = [ \delta, 1 - \delta]$ where $\delta$ is an arbitrarily small positive
constant.



% \vspace{10mm} Let $z$ be a univariate observation with a prior
% probability $\pi_0$ of being a member of class 0, and $1 - \pi_0 = \pi_1$ the
% prior probability of being a member of class 1.  Then for quantile level
% $\theta$, define
% \[
%   \rho\,(z, \theta) = \rho_{\theta}\Big(z,\, F_1^{-1}(\theta)\Big) -
%   \rho_{\theta}\Big(z,\, F_0^{-1}(\theta)\Big)
% \]
% Then the probability of a correct classification by the quantile classifier is
% given by
% \begin{equation}
%   \label{eq: theoretical rate}
%   \pi_0 \int \ind\Big( \rho(z, \theta) > 0 \Big)\, dP_0(z) + \pi_1 \int \ind\Big(
%   \rho(z, \theta) \leq 0 \Big) \,dP_1(z)
% \end{equation}
% The expression given in (\ref{eq:theoretical-rate}) has the appealing property
% that for the optimal choice of quantile level $\theta$ the classification rate is
% equal to that of the Bayes classifier.  Thus, to select the quantile level for
% the classification rule given by (\ref{eq:distance-classifier}), one approach
% is to select the empirically optimal choice of $\theta$, as determined by
% leave-one-out cross validation.  To give the definition of this estimator, we
% first describe the setting and introduce some preliminary definitions.

% Consider observations $x_{0, 1}, \dots, x_{0, n_0}$ sampled from population
% $\Pi_0$ and observations $x_{1, 1}, \dots, x_{1, n_1}$ sampled from population $\Pi_1$.
% Then we define
% \[
%   \widehat{\rho}(z, \theta) = \rho_{\theta}\Big(z,\,
%   \widehat{F}_1^{-1}(\theta)\Big) - \rho_{\theta}\Big(z,\,
%   \widehat{F}_0^{-1}(\theta)\Big)
% \]
% where $\widehat{F}_i^{-1}(\theta)$ is estimated from
% $x_{i,1}, x_{i,2}, \dots, x_{i,n_i},~ i = 1, 2$.  Further define
% $\widehat{\rho}_{-x_{i, j}} (z, \theta)$ to be as above, but where
% $\widehat{F}_i^{-1}(\theta)$ is estimated from the data after leaving out the
% $x_{i, j}$ observation (and the estimate of the comparison quantile is
% obtained using the full set of observations from that class).  Then we define
% the empirically optimal quantile level to be given by
% \begin{align}
%   \begin{split}
%     \widehat{\theta^{\text{opt}}}
%     &= \argmax_{\theta \in \{\theta_1, \dots, \theta_K\}} \left\{ \frac{n_0}{n_0 + n_1} \left[ \frac{1}{n_0} \sum_{z \in \Pi_0} \mathbbm{1}\Big( \widehat{\rho}_{-z}(z, \theta) > 0 \Big)  \right] +
%       \frac{n_1}{n_0 + n_1} \left[ \frac{1}{n_1} \sum_{z \in \Pi_1} \mathbbm{1}\Big( \widehat{\rho}_{-z}(z, \theta) \leq 0 \Big)  \right] \right\} \\[1ex]
%     &= \argmax_{\theta \in \{\theta_1, \dots, \theta_K\}} \left\{ \sum_{z \in \Pi_0} \mathbbm{1}\Big( \widehat{\rho}_{-z}(z, \theta) > 0 \Big) +
%       \sum_{z \in \Pi_1} \mathbbm{1}\Big( \widehat{\rho}_{-z}(z, \theta) \leq 0 \Big) \right\}
%   \end{split}
% \end{align}
% for $\theta_1, \dots, \theta_K$ a grid of quantile levels from which to compare.
% Conceptually, $\widehat{\theta^{\text{opt}}}$ is chosen to be the optimizer of
% the sample version of the theoretical optimizer given in equation
% (\ref{eq:theoretical-rate}).

% \begin{editornote}
%   This seems like a reasonable strategy and we have shown empirically that good
%   estimates for the optimal quantile level can be found for a moderate sample
%   size (see project update pdf for 2016-07-01).  However it does have some
%   drawbacks:
  
%   \begin{itemize}
%   \item adds computational burden to the method
%   \item eats up our sample size in that we should set aside training data to
%     train on the quantile estimates
%   \item practical issues with small sample sizes: seems unstable.  Depending on
%     the sample size and the proportion of observations, one class can end up
%     with just a handful or even no observations for a particular class.
%   \end{itemize}
  
% \end{editornote}


% \subsection{Choice of quantile level: distance-based}

% Another approach to selecting the quantile level for the classification rule
% given in expression (\ref{eq: distance classifier}) is based on comparing the
% distances between the quantiles for each class.  Intuitively, the quantiles with
% greater differences between the classes may contain more information in
% discriminating new observations through a comparison of their quantile distances
% to the quantile.  In this section we consider a class of composite
% distance-based quantile levels.

% One major drawback to selecting quantile levels based on a comparison of the
% distances between quantiles is the following.  Consider as an example the case
% where the observations are drawn from populations $\Pi_0$ and $\Pi_1$ such that
% the densities for the populations are $\normal(\mu_0, 1)$ and
% $\normal(\mu_1, 1)$, respectively.  Then
% $\big|F_1^{-1}(\theta) - F_0^{-1}(\theta)\big| = |\mu_1 - \mu_0|$ for all $\theta$. In
% this particular example, the optimal choice of quantile level is 0.5; however,
% when using a distance-based approach to selecting the quantile there is no
% meaningful information from which to make a decision, a clearly undesirable
% property.  However, when extending the classification rule to the multivariate
% setting, there is not a direct connection between the classification problem
% based on one variable and the classification problem drawing information from
% all of the variables, and in practice we may achieve better performance with
% this approach in some settings.

% We propose a class of composite distance-based quantile levels constructed as
% follows.  Consider a grid of quantile levels $\theta_1, \dots, \theta_K$.  Define
% \[
%   d_k = \left| \widehat{F}^{-1}_1 (\theta_k) - \widehat{F}^{-1}_0
%     (\theta_k) \,\right|, \hspace{5mm} k = 1, \dots, K
% \]
% Then a class of component-wise quantile weights is defined as
% \begin{equation}
%   w_k = \frac{ (d_k - \epsilon)_{+}^{\nu} }{ \sum_{\ell = 1}^K (d_{\ell}
%     - \epsilon)_{+}^{\nu} }, \hspace{5mm} k = 1, \dots, K.
% \end{equation}
% where $\nu$ and $\epsilon$ are nonnegative tuning parameters.
% % Various choices
% % of $\nu$ and $\epsilon$ lead to special cases such as
% % \begin{itemize}
% % \item $w_k = 1 / K$
% % \item $\displaystyle w_k = \frac{ (d_k - \epsilon)_+ }
% %   { \sum_{\ell=1}^K (d_{\ell} - \epsilon)_+ }$
% % \item $\displaystyle w_k = \frac{ d_{k}^2 }{ \sum_{\ell=1}^K d_{\ell}^2 }$
% % \end{itemize}
% When using composite quantile levels we write the classifier as follows.  For
% new observation $z$, classify to
% \begin{equation}
%   \label{eq: composite classifier}
%   \left\{ 
%     \begin{array}{lcl}
%       \Pi_0, & & \displaystyle \text{if} \hspace{3mm} \sum_{k=1}^K w_{jk}\,
%                  \bigg\{ \rho_{\theta_k} \left( z - \widehat{F}^{-1}_1 (\theta_k) \right) -
%                  \rho_{\theta_k} \left( z - \widehat{F}^{-1}_0 (\theta_k) \right) \bigg\}
%                  ~>~ 0 \\[2ex]
%       \Pi_1, & & \text{otherwise} \\[1ex]
%     \end{array}
%   \right.
% \end{equation}


% \subsection{Optimal quantile level estimation properties}
% \label{sec: opt quantile est}

% In this section we investigate the finite-sample properties of the leave-one-out
% cross validation approach for estimating the classification rate as a function
% of quantile level.
% \\
% ****  TODO: rework this section if we think it belongs in the paper.  *****
% \\
% \includegraphics[width=1\textwidth]{../../R_Scratch/Plots/Case1.pdf}



\subsection{OPTIMALITY AND CONSISTENCY OF  THE QUANTILE CLASSIFIER}
\label{sec:quantile-classifier-optimality}

Now that all of the definitions for the quantile classifier have been provided,
we may begin to study its properties.  We begin with the fundamental result for
the motivation of the paper: the quantile classifier achieves the Bayes rule
classification rate for the optimal choice of quantile level.

\begin{theorem}
  \label{thm:quantile-classifier-is-bayes}
  Consider two populations $\Pi_0$ and $\Pi_1$ with corresponding density
  functions $f_0$ and $f_1$ such that $f_0$ and $f_1$ are nonzero on the same
  domain.  Let $Z$ be a random variable with a prior probability $\pi_0$ of
  being a member of population $\Pi_0$, and $\pi_1 = 1 - \pi_0$ the prior
  probability of being a member of population $\Pi_1$.  Further assume that
  there is a point $z^{*}$ with $\pi_0\, f_0(z^{*}) = \pi_1\, f_1(z^{*})$ so
  that $\pi_0\, f_0(z) < \pi_1\, f_1(z)$ for $z$ on one size of $z^{*}$, and
  $\pi_0\, f_0(z) > \pi_1\, f_1(z)$ on the other side of $z^{*}$.  Then the
  quantile classifier for an observed realization of $Z$ using the optimal
  choice of quantile level achieves the Bayes rule misclassification
  probability.
\end{theorem}

\begin{proof}
  This result and proof were given as Lemma 2 in \cite{hennig2016}.
\end{proof}

Having established the theoretical optimality of the quantile classifier, we now
consider the consistency of the empirical version.  For the next lemma, we will
need the following assumptions.
\begin{enumerate}[label=\emph{Assumption \arabic*.}, align=left]
\item $F_i^{-1}$ is a continuous function of
  $\theta,~ i=0,1$.
\item For all $\theta \in T,~ \prob\Big( \Lambda(Z, \theta) = 0 \Big) = 0$.
\item There is a unique $\tilde{\theta}$ that satisfies $\tilde{\theta} =
  \argmax_{\theta \in T} \Psi(\theta)$.
\end{enumerate}

\begin{lemma}
  \label{lem:univariate-consistency}
  Let $\tilde{\theta}$ be a solution to
  $\tilde{\theta} = \argmax_{\theta \in T} \Psi(\theta)$.  Under Assumptions 1
  and 2, it follows that
  $\Psi(\hat{\theta}_n) \stackrel{p}{\longrightarrow} \Psi(\tilde{\theta})$.
  Furthermore, under Assumptions 1, 2, and 3, it follows that
  $\hat{\theta}_n \stackrel{p}{\longrightarrow} \tilde{\theta}$.
\end{lemma}

\begin{proof}
  The fact that
  $\Psi(\hat{\theta}_n) \stackrel{p}{\longrightarrow} \Psi(\tilde{\theta})$ is a
  special case of Theorem 1 in \cite{hennig2016} for a feature-space of
  dimension 1.  Furthermore, during the proof of that theorem it was shown that
  under Assumptions 1 and 2, $\Psi$ is a continuous function of $\theta$.  To
  show that $\hat{\theta}_n \stackrel{p}{\longrightarrow} \tilde{\theta}$
  suppose that the claim doesn't hold.  Then there exists an $\epsilon > 0$ and
  $\delta > 0$ such that for all $N \in \mathbb{N}$, there exists an $n \geq N$
  such that
  \begin{equation*}
    \prob\Big(
    \big| \hat{\theta}_n - \tilde{\theta} \big|
    > \epsilon \Big)
    \geq \delta.
  \end{equation*}
  Now, because $\Psi$ is continuous and $\Psi(\tilde{\theta})$ is a unique
  maximum, it follows that we can find $\nu > 0$ such that
  \begin{equation*}
    \min \left\{
      \big| \Psi(\Tilde{\theta} - \epsilon) - \Psi(\tilde{\theta}) \big|\,,
      \hspace{2mm}
      \big| \Psi(\Tilde{\theta} + \epsilon) - \Psi(\tilde{\theta}) \big|
    \right\} \geq \nu.
  \end{equation*}
  Therefore, for all $N \in \mathbb{N}$, there exists an $n \geq N$ such that
  \begin{equation*}
    \prob\Big(
    \big| \Psi(\hat{\theta}_n) - \Psi(\tilde{\theta}) \big|
    \geq \nu \Big) \geq
    \prob\Big(
    \big| \hat{\theta}_n - \tilde{\theta} \big|
    \geq \epsilon \Big)
    \geq \delta.
  \end{equation*}
  But this is in contradiction to the fact that
  $\Psi(\hat{\theta}_n) \stackrel{p}{\longrightarrow} \Psi(\tilde{\theta})$.
\end{proof}




\subsection{DECISION RULE CALCULATION}
\label{sec:empirical-quantile-classifier-results}

So it is established that the classification rate of the empirically optimal
quantile classifier converges to the classification rate of the true optimal
quantile classifier.  Next, we investigate the practical considerations of
calculating the empirically optimal quantile classifier.  Let us define
$F_{(0)}^{-1}(\theta) = \min\Big\{ F_0^{-1}(\theta),~ F_1^{-1}(\theta) \Big\}$,
$\Phi_{(0)}(z, \theta) = \rho_{\theta}\left(z - F_{(0)}^{-1}(\theta)\right)$,
$F_{(1)}^{-1}(\theta) = \max\Big\{ F_0^{-1}(\theta),~ F_1^{-1}(\theta) \Big\}$,
and
$\Phi_{(1)}(z, \theta) = \rho_{\theta}\left(z - F_{(1)}^{-1}(\theta)\right)$.

\begin{lemma}
  \label{lem:decision-boundary}
  If $F_{(0)}^{-1}(\theta) \ne F_{(1)}^{-1}(\theta)$, then the quantile
  classifier defined in equation (\ref{eq:quantile-classifier}) can be
  equivalently expressed as follows.
  \begin{equation}
    \label{eq:alt-quantile-classifier}
    \text{For an observation $z$, classify to:} \hspace{5mm} \left\{ 
      \begin{array}{lcl}
        \Pi_{(0)}, & & z ~<~ \theta F_{(0)}^{-1}(\theta) +
                       (1 - \theta)\, F_{(1)}^{-1}(\theta) \\[1ex]
        \Pi_{1}, & & z ~=~ \theta F_{(0)}^{-1}(\theta) +
                       (1 - \theta)\, F_{(1)}^{-1}(\theta) \\[1ex]
        \Pi_{(1)}, & & \textup{otherwise}
      \end{array}
    \right.
  \end{equation}
\end{lemma}

\begin{proof}
  Suppose $F_{(0)}^{-1}(\theta) \ne F_{(1)}^{-1}(\theta)$.  It is clear that
  $\Phi_{(0)}(z, \theta)$ is equal to $\Phi_{(1)}(z, \theta)$ at exactly one
  point, say $\tau$, and that the following holds:
  \begin{equation*}
    \left\{
      \begin{array}{lll}
        \Phi_{(0)}(z, \theta) < \Phi_{(1)}(z, \theta), & & z < \tau \\[1ex]
        \Phi_{(0)}(z, \theta) = \Phi_{(1)}(z, \theta), & & z = \tau \\[1ex]
        \Phi_{(0)}(z, \theta) > \Phi_{(1)}(z, \theta), & & z > \tau \\
      \end{array}
    \right.
  \end{equation*}
  Furthermore, we can infer that
  $F_{(0)}^{-1}(\theta) < \tau < F_{(1)}^{-1}(\theta)$.  Setting the loss
  functions equal for $z$ in this interval yields:
  \begin{align*}
    & \Phi_{(0)}(z, \theta) \stackrel{\mathit{set}}{=} \Phi_{(1)}(z, \theta) \\
    & \hspace{5mm} \Longleftrightarrow \hspace{5mm}
      \ind\!\! \left(z > F_{(0)}^{-1}(\theta)\right) \theta
      \left(z - F_{(0)}^{-1}(\theta)\right) +
      \ind\!\! \left(z \leq F_{(0)}^{-1}(\theta)\right) (1 - \theta)
      \left(F_{(0)}^{-1}(\theta) - z\right) \\
    & \hspace{25mm} =
      \ind\!\! \left(z > F_{(1)}^{-1}(\theta)\right) \theta
      \left(z - F_{(1)}^{-1}(\theta)\right) +
      \ind\!\! \left(z \leq F_{(1)}^{-1}(\theta)\right) (1 - \theta)
      \left(F_{(1)}^{-1}(\theta) - z\right) \\
    & \hspace{5mm} \Longleftrightarrow \hspace{5mm}
      \theta \left(z - F_{(0)}^{-1}(\theta)\right) =
      (1 - \theta) \left(F_{(1)}^{-1}(\theta) - z\right) \\
    & \hspace{5mm} \Longleftrightarrow \hspace{5mm}
      z = \theta\, F_{(0)}^{-1}(\theta) + (1 - \theta)\, F_{(1)}^{-1}(\theta).
  \end{align*}
\end{proof}
It is interesting to note that the decision rule boundary lies on the line
segment between $F_0^{-1}(\theta)$ and $F_1^{-1}(\theta)$, with the location of
the point determined by the quantile level.  The next result characterizes the
solution set for the empirical quantile level estimator.

\begin{lemma}
  \label{lem:empirical-quantlev}
  Let $x_1, \dots, x_m$ be points on $\mathbb{R}$.  Then a solution to equation
  (\ref{eq:empirical-quantile}) providing the $\theta$-th empirical quantile for
  $x_1, \dots, x_m$ is given by $\lceil m \theta \rceil$-th largest value of the
  $x_i$.
\end{lemma}

\begin{proof}
  \input{empirical_quant_proof.tex}
\end{proof}

\begin{lemma}
  \label{lem:decision-rule-time}
  Obtaining the quantile classifier decision rule boundary for fixed choice of
  quantile level is an $\bigO(n)$ operation.
\end{lemma}

\begin{proof}
  Constructing the $\theta$-th quantile classifier requires merely finding the
  $\lceil \theta n_0 \rceil$-th largest value from the observations that were
  drawn from population $\Pi_0$, and the $\lceil \theta n_1 \rceil$-th largest
  value from the observations that were drawn from population $\Pi_1$, and then
  calculating the decision boundary based on Lemma \ref{lem:decision-boundary}.
  Finding the $k$-th largest value of a set is an $\bigO(n)$ operation, and the
  calculation to obtain the decision boundary using Lemma
  \ref{lem:decision-boundary} is an $\bigO(1)$ operation.
\end{proof}




\subsection{CALCULATING THE EMPIRICALLY OPTIMAL QUANTILE CLASSIFIER}
\label{sec:empirically-optimal-algo}

Given the availability of a closed-form expression for the decision rule
boundary at a fixed quantile level, an immediately apparent question is whether
an empirically optimal quantile classifier can be efficiently obtained.
Intuitively, we can sort the data and simply ``slide'' the decision rule
boundary across the range of the data, counting the number of data points for
either class on the correct side of the boundary to find an optimal choice.
Furthermore, since the classification rate doesn't change as we slide the
decision boundary in-between observations, we need only calculate the
classification rate for some number of calculations that is bounded from above
by the number of observations.  Also, we note that calculating the
classification rate requires merely counting the number of observations for each
class on the correct side of the decision boundary (on sorted data), an
extremely cheap operation.

One complication of this basic strategy is that the set of decision rule
boundaries for a given data set does not span the real line, so a first step
that must be taken is to obtain this set of valid decision rule boundaries.  In
other words, let $\phi(\theta; \text{data}){:}~ (0, 1) \mapsto \mathbb{R}$ be
the function that maps a quantile level for a given data set to the quantile
classifier decision rule boundary; then we want to obtain the set
$\big\{ x \in \mathbb{R}{:} \hspace{3mm} \theta \in (0, 1),~ \phi(\theta;
\text{data}) = x \big\}$.  This set can be obtained by working backwards from
the data and by using the expressions that we have for the decision rule
boundary (i.e. Lemma \ref{lem:decision-boundary} and Lemma
\ref{lem:empirical-quantlev}).]

The key observation that facilitates the calculation of this set is that the
domain $(0, 1)$ can be partitioned into smaller intervals for which range of the
decision rule boundary function $\phi$ can be easily calculated.  In more
detail, consider an interval $(\theta_{\mathit{low}},\, \theta_{\mathit{upp}}]$
such that $\lceil \theta n_0 \rceil$ and $\lceil \theta n_1 \rceil$ are each
constant for all $\theta$ in the interval.  As a consequence of Lemma
\ref{lem:empirical-quantlev} , this implies that there are values $a$ and $b$
such that $F_{(0)}^{-1}(\theta) = a$ and $F_{(1)}^{-1}(\theta) = b$ for all
$\theta$ in the interval.  Since by Lemma \ref{lem:decision-boundary} we have
$\phi(\theta; \text{data}) = \theta a + (1 - \theta) b$ for any $\theta$ in the
interval we obtain that
$\Big\{ x \in \mathbb{R}{:} \hspace{3mm} \theta \in (\theta_{\mathit{low}},\,
\theta_{\mathit{upp}}],~ \phi(\theta; \text{data}) = x \Big\} = \Big(
\theta_{\mathit{upp}}\, a + (1 - \theta_{\mathit{upp}})\, b, \hspace{3mm}
\theta_{\mathit{low}}\, a + (1 - \theta_{\mathit{low}})\, b\, \Big]$.

It remains then to decide how to partition $(0, 1)$ into such sets.  For
notational convenience we actually consider $(0, 1]$: even though the 1-th
quantile doesn't make sense theoretically, its estimate is well-defined.  Now
$\lceil \theta n_0 \rceil$ is a step function with the endpoints for each step
taking place at $1, \dots, n_0$ and which occur for values of $\theta$ at
$1 / n_0, 2 / n_0, \dots, n_0 / n_0$.  Similarly, $\lceil \theta n_1 \rceil$ is
a step function with the endpoint for each step taking place at $1, \dots, n_1$
and which occurs for values of $\theta$ at $1 / n_1, 2 / n_1, \dots, n_1 / n_1$.
Thus any interval that doesn't include one of those values of $\theta$ except as
its upper bound is constant for each of these functions.  So we can partition
$(0, 1]$, by letting each interval have an open lower bound be one of the step
function change locations for one of the classes, and having the upper bound be
the next smallest step function change location for either of the classes (and
we also need an interval from 0 to the smallest step function change location).

At this point we have all of the concepts that we need to construct an algorithm
to calculate an empirically optimal choice of quantile level.  The algorithm is
presented as Algorithm \ref{alg:empirically-optimal-classifier}.

% \begin{enumerate}[label=\arabic*., align=left]
% \item sort $v_i \in \Pi_V$ and $w_i \in \Pi_W$
% \item calculate the set of quantile levels given by
%   \begin{equation*}
%     \Theta = \Big\{ 0 \Big\} \bigcup
%     \left\{ \frac{k}{n_0}: 1 \leq k \leq n_0 \right\} \bigcup
%     \left\{ \frac{k}{n_1}: 1 \leq k \leq n_1 \right\}
%   \end{equation*}
% \item sort $\Theta$
% \item for $i$ in 2 to $|\Theta|$; do
%   \begin{enumerate}[label=\alph*)]
%   \item calculate $\hat{F}_V^{-1}(\theta_i)$ and $\hat{F}_W^{-1}(\theta_i)$, and
%     find
%     \begin{equation*}
%       a = \min \Big\{ \hat{F}_V^{-1}(\theta_i),~ \hat{F}_W^{-1}(\theta_i) \Big\}
%       \hspace{5mm} \text{and} \hspace{5mm}
%       b = \max \Big\{ \hat{F}_V^{-1}(\theta_i),~ \hat{F}_W^{-1}(\theta_i) \Big\}
%     \end{equation*}
%   \item calculate the interval
%     $G_i = \Big(
%     \theta_i\, a + (1 - \theta_i)\, b,
%     \hspace{3mm}
%     \theta_{i-1}\, a + (1 - \theta_{i-1})\, b\,
%     \Big]$
%   \end{enumerate}
% \item calculate $H_1, \dots, H_r$ the collection of disjoint intervals formed by
%   joining overlapping intervals of $G_i$
% \item for $H_k$ in $H_1, \dots, H_r$; do
%   \begin{enumerate}[label=\alph*)]
%   \item set $x$ equal to the largest value from either class smaller then every
%     element in $H_k$;
%   \item calculate the classification rate for the decision rule at $\inf H_k$
%   \item set $x$ equal to the smallest value from either class greater than the
%     current $x$.  while $x \in H_k$; do
%     \begin{enumerate}[label=\roman*.]
%     \item calculate the classification rate for the decision rule at $x$
%     \end{enumerate}
%   \end{enumerate}
% \end{enumerate}

% \begin{enumerate}[label=\arabic*., align=left]
% \item sort $v_i \in \Pi_{0}$ and $w_i \in \Pi_{1}$
% \item calculate the set of quantile levels given by
%   \begin{equation*}
%     \Theta = \Big\{ 0 \Big\} \bigcup
%     \left\{ \frac{k}{n_0}: 1 \leq k \leq n_0 \right\} \bigcup
%     \left\{ \frac{k}{n_1}: 1 \leq k \leq n_1 \right\}
%   \end{equation*}
% \item sort $\Theta$
% \item for $i$ in 2 to $|\Theta|$; do
%   \begin{enumerate}[label=\alph*)]
%   \item calculate $\hat{F}_0^{-1}(\theta_i)$ and $\hat{F}_1^{-1}(\theta_i)$, and
%     find
%     \begin{equation*}
%       a = \min \Big\{ \hat{F}_0^{-1}(\theta_i),~ \hat{F}_1^{-1}(\theta_i) \Big\}
%       \hspace{5mm} \text{and} \hspace{5mm}
%       b = \max \Big\{ \hat{F}_0^{-1}(\theta_i),~ \hat{F}_1^{-1}(\theta_i) \Big\}
%     \end{equation*}
%     % Assume for ease of presentation that
%     % $\hat{F}_0^{-1}(\theta_i) < \hat{F}_1^{-1}(\theta_i)$.
%   \item calculate the interval
%     $G_i = \Big[
%     \theta_i\, a + (1 - \theta_i)\, b,
%     \hspace{3mm}
%     \theta_{i-1}\, a + (1 - \theta_{i-1})\, b\,
%     \Big)$
%   \item find a collection of sets $\mathcal{H}_i$ such that the classification
%     rate is constant for each set.
%     \begin{itemize}
%     \item
%       $[\text{lbnd}, x_k], (x_k, x_{k+1}], \dots, (x_{k+r-1}, x_{k+r}],
%       (x_{k+r}, \text{hbnd})$
%     \item
%       $[\text{lbnd}, x_k), [x_k, x_{k+1}), \dots, [x_{k+r-1}, x_{k+r}),
%       [x_{k+r}, \text{hbnd})$
%     \end{itemize}
%   % \item find the largest value from $\Pi_{(0)}$ less than or equal to
%   %   $\inf H_k$, call it $x$, and the smallest value from $\Pi_{(1)}$ greater
%   %   than $\inf H_k$, call it $y$, and let $\tau = y$
%   % \item count the number of observations from $\Pi_{(0)}$ less than or
%   %   equal to $x$, and the number of observations from $\Pi_{(1)}$ greater than
%   %   or equal to $y$ to obtain the classification rate for a small value in
%   %   $G_i$
%   \item for each set $H_j$ in $\mathcal{H}_i$; do
%     \begin{enumerate}[label=\roman*.]
%     \item calculate the classification rate by counting the number of
%       observations from each class to to the correct side of an arbitrary point
%       in $H_i$
%     \item map the set $H_i$ back to the set of corresponding quantiles on $(0, 1]$
%     \end{enumerate}
%   \end{enumerate}
% \end{enumerate}

% \begin{enumerate}[label=\arabic*., align=left]
% \item sort $v_i \in \Pi_{0}$ and $w_i \in \Pi_{1}$
% \item calculate the set of quantile levels given by
%   \begin{equation*}
%     \Theta = \Big\{ 0 \Big\} \bigcup
%     \left\{ \frac{k}{n_0}: 1 \leq k \leq n_0 \right\} \bigcup
%     \left\{ \frac{k}{n_1}: 1 \leq k \leq n_1 \right\}
%   \end{equation*}
% \item sort $\Theta$
% \item for $i$ in 2 to $|\Theta|$; do
%   \begin{enumerate}[label=\alph*)]
%   \item calculate $\hat{F}_0^{-1}(\theta_i)$ and $\hat{F}_1^{-1}(\theta_i)$, and
%     find
%     \begin{equation*}
%       a = \min \Big\{ \hat{F}_0^{-1}(\theta_i),~ \hat{F}_1^{-1}(\theta_i) \Big\}
%       \hspace{5mm} \text{and} \hspace{5mm}
%       b = \max \Big\{ \hat{F}_0^{-1}(\theta_i),~ \hat{F}_1^{-1}(\theta_i) \Big\}
%     \end{equation*}
%     % Assume for ease of presentation that
%     % $\hat{F}_0^{-1}(\theta_i) < \hat{F}_1^{-1}(\theta_i)$.
%   \item calculate the interval
%     $G_i = \Big(
%     \theta_i\, a + (1 - \theta_i)\, b,
%     \hspace{3mm}
%     \theta_{i-1}\, a + (1 - \theta_{i-1})\, b\,
%     \Big]$
%   \item find the largest value from $\Pi_{(0)}$ less than or equal to
%     $\inf H_k$, call it $x$, and the smallest value from $\Pi_{(1)}$ greater
%     than $\inf H_k$, call it $y$, and let $\tau = y$
%   \item count the number of observations from $\Pi_{(0)}$ less than or
%     equal to $x$, and the number of observations from $\Pi_{(1)}$ greater than
%     or equal to $y$ to obtain the classification rate for a small value in
%     $G_i$
%   \item while there exists an index $i$ such that either
%     $\tau < v_i \leq \max(G_i)$ or $\tau < w_i \leq \max(G_i)$; do
%     \begin{enumerate}[label=\roman*.]
%     \item let $\tau$ assume the smallest value $v_i$ or $w_i$ greater than the
%       previous value of $\tau$
%     \item calculate the classification rate for the decision rule boundary at
%       $\tau$ by adding to or subtracting from the correct classification rate
%       from the previous calculation depending on whether shifting the boundary
%       switches values from the correct to the incorrect side of the boundary or
%       vice versa
%     \end{enumerate}
%   \end{enumerate}
% \end{enumerate}

% \begin{lemma}
%   The decision rule for the empirically optimal quantile classifier can be
%   obtained in $\mathcal{O}(n \log n)$ time.
% \end{lemma}

% \begin{proof}
%   Sorting the data is an $\mathcal{O}(n \log n)$ operation.  Steps 2-3 each take
%   $\mathcal{O}(n)$ time.  Step 4 requires for each iteration estimating
%   $F_V^{-1}$ and $F_W^{-1}$ and then calculating the interval $G_i$ each of
%   which is a constant time operation, for a number of iterations bounded from
%   above by $n - 1$.  Step 5 requires a constant time operation for a number of
%   sets bounded from above by $n - 1$.

%   For step 5, essentially for each iteration
%   finding the initial values of $v_i$ and $w_j$ are a $\mathcal{O}(\log n)$
%   operation, and the sum of the operations in step 5c is an $\mathcal{O}(n)$
%   operation.
% \end{proof}

\begin{algorithm}[p]
  \label{alg:empirically-optimal-classifier}
  \DontPrintSemicolon
  \BlankLine
  % \KwResult{Write here the result}
  \SetKwInOut{Input}{Data}
  % \SetKwInOut{Output}{Output}
  \Input{$v_1, \dots, v_{n_0}$ from $\Pi_0$ and $w_1, \dots, w_{n_1}$ in
    $\Pi_W$}
  % \Output{Write here the output}
  % \BlankLine
  
  sort observations $v_1, \dots, v_{n_0}$ and $w_1, \dots, w_{n_1}$.  Assume
  that in the case of ties, we classify to $\Pi_W$. \;
  
  sort $x_1, \dots, x_r$ a collection of the unique values from among the $v_i$
  and $w_i$ \;
  
  calculate the set of quantile levels given by
  \begin{equation*}
    \Theta = \Big\{ 0 \Big\} \bigcup
    \left\{ \frac{k}{n_0}: 1 \leq k \leq n_0 \right\} \bigcup
    \left\{ \frac{k}{n_1}: 1 \leq k \leq n_1 \right\}
  \end{equation*}
  so that $\Theta$ is sorted \;
  \BlankLine

  \ForEach{$i$ in 2 to $|\Theta|$}{
    
    \BlankLine
    
    calculate $\hat{F}_V^{-1}(\theta_i)$ and $\hat{F}_W^{-1}(\theta_i)$, and
    find
    \begin{equation*}
      a = \min \Big\{ \hat{F}_0^{-1}(\theta_i),~ \hat{F}_1^{-1}(\theta_i) \Big\}
      \hspace{5mm} \text{and} \hspace{5mm}
      b = \max \Big\{ \hat{F}_0^{-1}(\theta_i),~ \hat{F}_1^{-1}(\theta_i) \Big\}
    \end{equation*}

    calculate the interval
    \[
      G_i = \Big[
      \theta_i\, a + (1 - \theta_i)\, b,
      \hspace{3mm}
      \theta_{i-1}\, a + (1 - \theta_{i-1})\, b\,
      \Big) := \big[x_{\scriptscriptstyle\text{low}},\,
      x_{\scriptscriptstyle\text{high}}\big)
    \]
    % Let $x_{\scriptscriptstyle\text{low}}$ and
    % $x_{\scriptscriptstyle\text{high}}$ denote the lower and upper bounds of $G_i$,
    % respectively.  \;

    find the smallest $x_i \in [x_{\scriptscriptstyle\text{low}}, \infty)$.  If
    $x_i >= x_{\scriptscriptstyle\text{high}}$ then just calculate the
    classification rate for $G_i$.

    \BlankLine

    \While{$x_i \in G_i$}{

      calculate the classification rate for the interval with upper and lower
      bounds determined by $x_i$ and
      $\min\{x_{i+1}, x_{\scriptscriptstyle\text{high}}\}$, respectively (for
      whether bounds in interval are open or closed see below)

      \BlankLine

      \uIf{$\hat{F}_V^{-1}(\theta_i) < \hat{F}_V^{-1}(\theta_i)$}{
        % partition $G_i$ into the following intervals each with a constant
        % classification rate \vspace{-2mm}
        map current interval to one of the following for some $s$:
        \begin{equation*}
          [x_{\scriptscriptstyle\text{low}}, x_k],\, (x_k, x_{k+1}],\, \dots,\,
          (x_{k+s-1}, x_{k+s}],\, (x_{k+s},\, x_{\scriptscriptstyle\text{high}})
        \end{equation*}
      }
      \Else{
        % partition $G_i$ into the following intervals each with a constant
        % classification rate \vspace{-2mm}
        map current interval to one of the following for some $s$:
        \begin{equation*}
          [x_{\scriptscriptstyle\text{low}}, x_k),\, [x_k, x_{k+1}),\, \dots,\,
          [x_{k+s-1}, x_{k+s}),\, [x_{k+s},\, x_{\scriptscriptstyle\text{high}})
        \end{equation*}
      }

      map interval back to corresponding interval in the quantile levels space \;
      
      $x_i \leftarrow x_{i+1}$
            
    }

    % \uIf{$\hat{F}_V^{-1}(\theta_i) < \hat{F}_V^{-1}(\theta_i)$}{
    %   partition $G_i$ into the following intervals each with a constant
    %   classification rate \vspace{-2mm}
    %   \begin{equation*}
    %     [x_{\scriptscriptstyle\text{low}}, x_k],\, (x_k, x_{k+1}],\, \dots,\,
    %     (x_{k+s-1}, x_{k+s}],\, (x_{k+s},\, x_{\scriptscriptstyle\text{high}})
    %   \end{equation*}
    % }
    % \uElse{
    %   partition $G_i$ into the following intervals each with a constant
    %   classification rate \vspace{-2mm}
    %   \begin{equation*}
    %     [x_{\scriptscriptstyle\text{low}}, x_k),\, [x_k, x_{k+1}),\, \dots,\,
    %     [x_{k+s-1}, x_{k+s}),\, [x_{k+s},\, x_{\scriptscriptstyle\text{high}})
    %   \end{equation*}
    % }  
    
  }
  \caption{calculate the empirically optimal quantile classifer}
\end{algorithm}


A set of optimal decision rule boundaries and corresponding quantile levels is
obtained by performing above algorithm, so the empirically optimal quantile
classifier can be efficiently calculated.  In practice, we might be willing to
approximate the empirically optimal quantile classifier by simply calculating
the empirical classification rate over a grid of quantile levels.  This reduces
the complexity to an $\mathcal{O}(Kn)$ operation, where $K$ is the number of
points in the grid.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "cqc_paper"
%%% End:
