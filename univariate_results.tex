
\section{Univariate classification rules}
\label{sec:univariate-classifier}

Quantile-based classifiers such as Hall et al.'s median-based classifiers and
Hennig and Viroli's quantile-based classifiers belong to a class of
distance-based classification methods that classify a data observation based on
the collective information provided by each of the marginal component-wise
distances for the observation to of the potential member classes' components
based on some metric of distance.  The methods proposed in this paper are built
upon this same idea, and in this section we introduce some notation and consider
the univariate setting before moving on to the multivariate setting in later
sections.

The quantile classifier and the empirically optimal quantile classifier was
introduced in Hennig and Viroli (2016).  For completeness, the notation and
definitions are reintroduced in Sections \ref{sec:quantile-classifier} and
\ref{sec:empirical-classifier}.


\subsection{The quantile classifier}
\label{sec:quantile-classifier}

Consider the so-called check loss (also called the quantile loss) function
defined as
\begin{align}
  \label{eq:check-loss}
  \begin{split}
    \rho_\theta(u)
    &= \ind(u > 0)\, \theta\, u  + \ind (u \leq 0)\, (1 - \theta)\, (-u)  \\[0ex]
    &= u\, \big[ \theta - \ind(u \leq 0) \big]
  \end{split}
\end{align}
\\[-1ex]
for some choice of $\theta \in (0,1)$.  This loss function has the important
property that for the $\theta^{\text{th}}$ quantile level of a continuous
univariate random variable $X$ it induces the $\theta^{\text{th}}$ quantile as
the minimizer of the expected distance to $X$.  That is,
\begin{equation}
  \label{eq:check-loss-min}
  \argmin_q \ev \big[ \rho_\theta (X - q) \big] = F_X^{-1}(\theta)
\end{equation}
for $F_X$ the distribution function of $X$.  Next, consider two populations
$\Pi_0$ and $\Pi_1$ with probability densities $P_0$ and $P_1$ on $\mathbb{R}$.
We define the quantile distance of a point $z$ to the population's $\theta$-th
quantile as
\begin{equation}
  \label{eq:phi}
  \Phi_i(z, \theta) = \rho_{\theta}\Big(z - F_i^{-1}(\theta)\Big),
  \hspace{5mm} i = 0, 1.
\end{equation}
Then the difference of the quantile distances of a point $z$ to the populations'
$\theta$-th quantiles is defined as
\begin{equation}
  \Lambda\,(z, \theta) = \Phi_1(z, \theta) - \Phi_0(z, \theta).
\end{equation}
The $\theta$-th quantile-based classifier is defined as follows.
\begin{equation}
  \label{eq:quantile-classifier}
  \text{For an observation $z$, classify to:} \hspace{5mm} \left\{ 
    \begin{array}{lcl}
      \Pi_0, & & \text{if} \hspace{3mm}\Lambda\,(z, \theta) > 0 \\[0ex]
      \Pi_1, & & \text{otherwise}
    \end{array}
  \right.
\end{equation}
Finally, let $Z$ be a random variable with a prior probability $\pi_0$ of being
a member of population $\Pi_0$, and $1 - \pi_0 = \pi_1$ the prior probability of
being a member of population $\Pi_1$.  Then the probability of correctly
classifying an observed realization of $Z$ by the $\theta$-th quantile
classifier is given by
\begin{equation}
  \label{eq:theoretical-rate}
  \Psi(\theta) =
  \pi_0 \int \ind\Big( \Lambda(z, \theta) > 0 \Big)\, dP_0(z) +
  \pi_1 \int \ind\Big(\Lambda(z, \theta) \leq 0 \Big)\, dP_1(z)
\end{equation}
The expression given in (\ref{eq:theoretical-rate}) has the appealing property
that for the optimal choice of quantile level $\theta$ the classification rate is
equal to that of the Bayes classifier \citep{hennig2016}.


% Next, let $x_1, \dots, x_n$ be samples from a population with a probability
% density on $\mathbb{R}$.  Then we define the $\theta^{\text{th}}$ empirical
% quantile for the population to be given by minimizer of the sample version of
% equation (\ref{eq: check loss min}) defined by:

% Next, let $\Pi_0$ and $\Pi_1$ be
% two populations with probability densities $P_0$ and $P_1$ on $\mathbb{R}$.
% Consider observations $x_{0, 1}, \dots, x_{0, n_0}$ sampled from population
% $\Pi_0$ and observations $x_{1, 1}, \dots, x_{1, n_1}$ sampled from population $\Pi_1$.
% Then we define $\widehat{F}^{-1}_i (\theta)$ to be the $\theta^{\text{th}}$
% empirical quantile for population $\Pi_i$, where
% \begin{equation}
%   \label{eq: empirical quantile}
%   \widehat{F}^{-1} (\theta) = \argmin_q \left\{ \theta \sum_{ x_{i} > q } |x_{i} - q| ~+~
%     (1 - \theta) \sum_{ x_{i} \leq q } |x_{i} - q| \right\}.
% \end{equation}
% Thus, the empirical quantile is the the minimizer of the sample version of
% equation (\ref{eq: check loss min}).  Finally, let $z$ be a new observation.
% Then classify $z$ to
% \begin{equation}
%   \label{eq: distance classifier}
%   \left\{ 
%     \begin{array}{lcl}
%       \Pi_0, & & \text{if} \hspace{3mm} \rho_{\theta} \left( z - \widehat{F}^{-1}_1 (\theta) \right) ~-~
%                  \rho_{\theta} \left( z - \widehat{F}^{-1}_0 (\theta) \right) ~>~ 0 \\[0ex]
%       \Pi_1, & & \text{otherwise}
%     \end{array}
%   \right.
% \end{equation}
% Conceptually, classify the observation to the population for which it has the
% smaller quantile distance to from the $\theta^{\text{th}}$ quantile.

% To use the classification rule given in equation (\ref{eq: distance classifier})
% in practice, it remains to select a choice of quantile level $\theta$.  In the
% following sections we consider two approaches for selecting the quantile level,
% with an eye towards the extension of the classification rule to the multivariate
% setting.


\subsection{The empirically optimal quantile classifier}
\label{sec:empirical-classifier}

Let $x_1, \dots, x_m$ be samples from a population with a probability density on
$\mathbb{R}$.  Then we define the $\theta^{\text{th}}$ empirical quantile for
the population to be given by the minimizer of the sample version of equation
(\ref{eq:check-loss-min}) defined by
\begin{equation}
  \label{eq:empirical-quantile}
  \widehat{F}^{-1} (\theta) = \argmin_q \left\{
    \theta \sum_{ x_{i} > q } |x_{i} - q| ~+~
    (1 - \theta) \sum_{ x_{i} \leq q } |x_{i} - q|
  \right\}.
\end{equation}
Then the empirical difference of the quantile distances of a point $z$ to the
population's $\theta$-th quantiles is defined as
\[
  \Lambda_n (z, \theta) = \rho_{\theta}\Big(z - \widehat{F}_{1n}^{-1}(\theta)\Big) -
  \rho_{\theta}\Big(z - \widehat{F}_{0n}^{-1}(\theta)\Big).
\]
where $\widehat{F}_{1n}$ is the estimate of the $\theta$-th quantile level of
population $\Pi_1$ based on the samples in $x_1, \dots, x_n$ that came from
population $\Pi_1$, and similarly for $\widehat{F}_{0n}$.  Let $n_0$ and $n_1$
be the number of observations from $\Pi_0$, and $\Pi_1$, respectively.  Then we
further define the observed rate of correct classification for the $\theta$-th
quantile as
\begin{align}
  \begin{split}
    \Psi_n(\theta)
    &= \frac{n_0}{n_0 + n_1} \left[
      \frac{1}{n_0} \sum_{x_i \in \Pi_0}
      \mathbbm{1}\Big( \Lambda_n(x_i, \theta) > 0 \Big)
    \right] +
      \frac{n_1}{n_0 + n_1} \left[
        \frac{1}{n_1} \sum_{x_i \in \Pi_1}
        \mathbbm{1}\Big( \Lambda_n(x_i, \theta) \leq 0 \Big)
      \right] \\[1ex]
    &= \frac{1}{n} \left[
      \sum_{x_i \in \Pi_0} \mathbbm{1}\Big( \Lambda_n(x_i, \theta) > 0 \Big) +
      \sum_{x_i \in \Pi_1} \mathbbm{1}\Big( \Lambda_n(x_i, \theta) \leq 0 \Big)
    \right].
  \end{split}
\end{align}
Finally, define the empirically optimal quantile classifier to be any solution
to the equation
\begin{equation}
  \hat{\theta}_n = \argmax_{\theta \in T} \Psi_n(\theta)
\end{equation}
for $T = [ \delta, 1 - \delta]$ where $\delta$ is an arbitrarily small positive
constant.



% \vspace{10mm} Let $z$ be a univariate observation with a prior
% probability $\pi_0$ of being a member of class 0, and $1 - \pi_0 = \pi_1$ the
% prior probability of being a member of class 1.  Then for quantile level
% $\theta$, define
% \[
%   \rho\,(z, \theta) = \rho_{\theta}\Big(z,\, F_1^{-1}(\theta)\Big) -
%   \rho_{\theta}\Big(z,\, F_0^{-1}(\theta)\Big)
% \]
% Then the probability of a correct classification by the quantile classifier is
% given by
% \begin{equation}
%   \label{eq: theoretical rate}
%   \pi_0 \int \ind\Big( \rho(z, \theta) > 0 \Big)\, dP_0(z) + \pi_1 \int \ind\Big(
%   \rho(z, \theta) \leq 0 \Big) \,dP_1(z)
% \end{equation}
% The expression given in (\ref{eq:theoretical-rate}) has the appealing property
% that for the optimal choice of quantile level $\theta$ the classification rate is
% equal to that of the Bayes classifier.  Thus, to select the quantile level for
% the classification rule given by (\ref{eq:distance-classifier}), one approach
% is to select the empirically optimal choice of $\theta$, as determined by
% leave-one-out cross validation.  To give the definition of this estimator, we
% first describe the setting and introduce some preliminary definitions.

% Consider observations $x_{0, 1}, \dots, x_{0, n_0}$ sampled from population
% $\Pi_0$ and observations $x_{1, 1}, \dots, x_{1, n_1}$ sampled from population $\Pi_1$.
% Then we define
% \[
%   \widehat{\rho}(z, \theta) = \rho_{\theta}\Big(z,\,
%   \widehat{F}_1^{-1}(\theta)\Big) - \rho_{\theta}\Big(z,\,
%   \widehat{F}_0^{-1}(\theta)\Big)
% \]
% where $\widehat{F}_i^{-1}(\theta)$ is estimated from
% $x_{i,1}, x_{i,2}, \dots, x_{i,n_i},~ i = 1, 2$.  Further define
% $\widehat{\rho}_{-x_{i, j}} (z, \theta)$ to be as above, but where
% $\widehat{F}_i^{-1}(\theta)$ is estimated from the data after leaving out the
% $x_{i, j}$ observation (and the estimate of the comparison quantile is
% obtained using the full set of observations from that class).  Then we define
% the empirically optimal quantile level to be given by
% \begin{align}
%   \begin{split}
%     \widehat{\theta^{\text{opt}}}
%     &= \argmax_{\theta \in \{\theta_1, \dots, \theta_K\}} \left\{ \frac{n_0}{n_0 + n_1} \left[ \frac{1}{n_0} \sum_{z \in \Pi_0} \mathbbm{1}\Big( \widehat{\rho}_{-z}(z, \theta) > 0 \Big)  \right] +
%       \frac{n_1}{n_0 + n_1} \left[ \frac{1}{n_1} \sum_{z \in \Pi_1} \mathbbm{1}\Big( \widehat{\rho}_{-z}(z, \theta) \leq 0 \Big)  \right] \right\} \\[1ex]
%     &= \argmax_{\theta \in \{\theta_1, \dots, \theta_K\}} \left\{ \sum_{z \in \Pi_0} \mathbbm{1}\Big( \widehat{\rho}_{-z}(z, \theta) > 0 \Big) +
%       \sum_{z \in \Pi_1} \mathbbm{1}\Big( \widehat{\rho}_{-z}(z, \theta) \leq 0 \Big) \right\}
%   \end{split}
% \end{align}
% for $\theta_1, \dots, \theta_K$ a grid of quantile levels from which to compare.
% Conceptually, $\widehat{\theta^{\text{opt}}}$ is chosen to be the optimizer of
% the sample version of the theoretical optimizer given in equation
% (\ref{eq:theoretical-rate}).

% \begin{editornote}
%   This seems like a reasonable strategy and we have shown empirically that good
%   estimates for the optimal quantile level can be found for a moderate sample
%   size (see project update pdf for 2016-07-01).  However it does have some
%   drawbacks:
  
%   \begin{itemize}
%   \item adds computational burden to the method
%   \item eats up our sample size in that we should set aside training data to
%     train on the quantile estimates
%   \item practical issues with small sample sizes: seems unstable.  Depending on
%     the sample size and the proportion of observations, one class can end up
%     with just a handful or even no observations for a particular class.
%   \end{itemize}
  
% \end{editornote}


% \subsection{Choice of quantile level: distance-based}

% Another approach to selecting the quantile level for the classification rule
% given in expression (\ref{eq: distance classifier}) is based on comparing the
% distances between the quantiles for each class.  Intuitively, the quantiles with
% greater differences between the classes may contain more information in
% discriminating new observations through a comparison of their quantile distances
% to the quantile.  In this section we consider a class of composite
% distance-based quantile levels.

% One major drawback to selecting quantile levels based on a comparison of the
% distances between quantiles is the following.  Consider as an example the case
% where the observations are drawn from populations $\Pi_0$ and $\Pi_1$ such that
% the densities for the populations are $\normal(\mu_0, 1)$ and
% $\normal(\mu_1, 1)$, respectively.  Then
% $\big|F_1^{-1}(\theta) - F_0^{-1}(\theta)\big| = |\mu_1 - \mu_0|$ for all $\theta$. In
% this particular example, the optimal choice of quantile level is 0.5; however,
% when using a distance-based approach to selecting the quantile there is no
% meaningful information from which to make a decision, a clearly undesirable
% property.  However, when extending the classification rule to the multivariate
% setting, there is not a direct connection between the classification problem
% based on one variable and the classification problem drawing information from
% all of the variables, and in practice we may achieve better performance with
% this approach in some settings.

% We propose a class of composite distance-based quantile levels constructed as
% follows.  Consider a grid of quantile levels $\theta_1, \dots, \theta_K$.  Define
% \[
%   d_k = \left| \widehat{F}^{-1}_1 (\theta_k) - \widehat{F}^{-1}_0
%     (\theta_k) \,\right|, \hspace{5mm} k = 1, \dots, K
% \]
% Then a class of component-wise quantile weights is defined as
% \begin{equation}
%   w_k = \frac{ (d_k - \epsilon)_{+}^{\nu} }{ \sum_{\ell = 1}^K (d_{\ell}
%     - \epsilon)_{+}^{\nu} }, \hspace{5mm} k = 1, \dots, K.
% \end{equation}
% where $\nu$ and $\epsilon$ are nonnegative tuning parameters.
% % Various choices
% % of $\nu$ and $\epsilon$ lead to special cases such as
% % \begin{itemize}
% % \item $w_k = 1 / K$
% % \item $\displaystyle w_k = \frac{ (d_k - \epsilon)_+ }
% %   { \sum_{\ell=1}^K (d_{\ell} - \epsilon)_+ }$
% % \item $\displaystyle w_k = \frac{ d_{k}^2 }{ \sum_{\ell=1}^K d_{\ell}^2 }$
% % \end{itemize}
% When using composite quantile levels we write the classifier as follows.  For
% new observation $z$, classify to
% \begin{equation}
%   \label{eq: composite classifier}
%   \left\{ 
%     \begin{array}{lcl}
%       \Pi_0, & & \displaystyle \text{if} \hspace{3mm} \sum_{k=1}^K w_{jk}\,
%                  \bigg\{ \rho_{\theta_k} \left( z - \widehat{F}^{-1}_1 (\theta_k) \right) -
%                  \rho_{\theta_k} \left( z - \widehat{F}^{-1}_0 (\theta_k) \right) \bigg\}
%                  ~>~ 0 \\[2ex]
%       \Pi_1, & & \text{otherwise} \\[1ex]
%     \end{array}
%   \right.
% \end{equation}


% \subsection{Optimal quantile level estimation properties}
% \label{sec: opt quantile est}

% In this section we investigate the finite-sample properties of the leave-one-out
% cross validation approach for estimating the classification rate as a function
% of quantile level.
% \\
% ****  TODO: rework this section if we think it belongs in the paper.  *****
% \\
% \includegraphics[width=1\textwidth]{../../R_Scratch/Plots/Case1.pdf}



\subsection{Quantile classifier results}
\label{sec:quantile-classifier-results}

Let us define
$F_{(0)}^{-1}(\theta) = \min\Big\{ F_0^{-1}(\theta),~ F_1^{-1}(\theta) \Big\}$
and
$F_{(1)}^{-1}(\theta) = \max\Big\{ F_0^{-1}(\theta),~ F_1^{-1}(\theta) \Big\}$.
% Also define Phi?

\begin{lemma}
  \label{lem:decision-boundary}
  The quantile classifier defined in equation (\ref{eq:quantile-classifier}) can
  be equivalently expressed as follows.
  \begin{equation}
    \label{eq:alt-quantile-classifier}
    \text{For an observation $z$, classify to:} \hspace{5mm} \left\{ 
      \begin{array}{lcl}
        \Pi_{(0)}, & & z ~<~ \theta F_{(0)}^{-1}(\theta) +
                       (1 - \theta)\, F_{(1)}^{-1}(\theta) \\[1ex]
        \Pi_{1}, & & z ~=~ \theta F_{(0)}^{-1}(\theta) +
                       (1 - \theta)\, F_{(1)}^{-1}(\theta) \\[1ex]
        \Pi_{(1)}, & & \textup{otherwise}
      \end{array}
    \right.
  \end{equation}
\end{lemma}

\begin{proof}
  It is clear that $\Phi_{(0)}(z, \theta)$ is equal to $\Phi_{(1)}(z, \theta)$
  at exactly one point, say $\tau$, and that the following holds:
  \begin{equation*}
    \left\{
      \begin{array}{lll}
        \Phi_{(0)}(z, \theta) < \Phi_{(1)}(z, \theta), & & z < \tau \\[1ex]
        \Phi_{(0)}(z, \theta) = \Phi_{(1)}(z, \theta), & & z = \tau \\[1ex]
        \Phi_{(0)}(z, \theta) > \Phi_{(1)}(z, \theta), & & z > \tau \\
      \end{array}
    \right.
  \end{equation*}
  Setting the loss functions equal for
  $F_{(0)}^{-1}(\theta) < z < F_{(1)}^{-1}(\theta)$ yields
  $z = \theta F_{(0)}^{-1}(\theta) + (1 - \theta)\, F_{(1)}^{-1}(\theta)$.
\end{proof}
It is interesting to note that the decision rule boundary lies on the line
segment between $F_0^{-1}(\theta)$ and $F_1^{-1}(\theta)$, with the location of
the point determined by the quantile level.  The next result characterizes the
solution set for the empirical quantile level estimator.

\begin{lemma}
  \label{lem:empirical-quantlev}
  Let $x_1, \dots, x_m$ be points on $\mathbb{R}$.  Then a solution to equation
  (\ref{eq:empirical-quantile}) providing the $\theta$-th empirical quantile for
  $x_1, \dots, x_m$ is given by $\lceil m \theta \rceil$-th largest value of the
  $x_i$.
\end{lemma}

\begin{proof}
  The proof for Lemma \ref{lem:empirical-quantlev} is somewhat tedious and is
  relegated to the supplementary materials.
\end{proof}
So to construct the $\theta$-th quantile classifier requires merely finding the
$(\theta n_0)$-th value from the observations $x_i$ that were drawn from
population $\Pi_0$, and the $(\theta n_1)$-th value from the observations $x_i$
that were drawn from population $\Pi_1$, and then calculating the decision
boundary based on Lemma \ref{lem:decision-boundary}.  This is an
$\mathcal{O}(n)$ operation.

For the next lemma, we will need the following assumptions.
\begin{enumerate}[label=\emph{Assumption \arabic*.}, align=left]
\item $F_i^{-1}$ is a continuous function of
  $\theta,~ i=0,1$.
\item For all $\theta \in T,~ \prob\Big( \Lambda(Z, \theta) = 0 \Big) = 0$.
\item There is a unique $\tilde{\theta}$ that satisfies $\tilde{\theta} =
  \argmax_{\theta \in T} \Psi(\theta)$.
\end{enumerate}

\begin{lemma}
  \label{lem:univariate-consistency}
  Let $\tilde{\theta}$ be a solution to
  $\tilde{\theta} = \argmax_{\theta \in T} \Psi(\theta)$.  Under Assumptions 1
  and 2, it follows that
  $\Psi(\hat{\theta}_n) \stackrel{p}{\longrightarrow} \Psi(\tilde{\theta})$.  Under
  Assumptions 1, 2, and 3, it follows that
  $\hat{\theta}_n \stackrel{p}{\longrightarrow} \tilde{\theta}$.
\end{lemma}

\begin{proof}
  The fact that
  $\Psi(\hat{\theta}_n) \stackrel{p}{\longrightarrow} \Psi(\tilde{\theta})$ is a
  special case of Theorem 1 in Hennig and Viroli for a feature-space of
  dimension 1.  Furthermore, during the proof of this theorem it was shown that
  under Assumptions 1 and 2, $\Psi$ is a continuous function of $\theta$.  To
  show that $\hat{\theta}_n \stackrel{p}{\longrightarrow} \tilde{\theta}$
  suppose that the claim doesn't hold.  Then there exists an $\epsilon > 0$ and
  $\delta > 0$ such that for all $N \in \mathbb{N}$, there exists an $n \geq N$
  such that
  \begin{equation*}
    \prob\Big(
    \big| \hat{\theta}_n - \tilde{\theta} \big|
    > \epsilon \Big)
    \geq \delta.
  \end{equation*}
  Now, because $\Psi$ is continuous and $\Psi(\tilde{\theta})$ is a unique
  maximum, it follows that we can find $\nu > 0$ such that
  \begin{equation*}
    \min \left\{
      \big| \Psi(\Tilde{\theta} - \epsilon) - \Psi(\tilde{\theta}) \big|\,,
      \hspace{3mm}
      \big| \Psi(\Tilde{\theta} + \epsilon) - \Psi(\tilde{\theta}) \big|
    \right\} \geq \nu.
  \end{equation*}
  Therefore, for all $N \in \mathbb{N}$, there exists an $n \geq N$ such that
  \begin{equation*}
    \prob\Big(
    \big| \Psi(\hat{\theta}_n) - \Psi(\tilde{\theta}) \big|
    \geq \nu \Big) \geq
    \prob\Big(
    \big| \hat{\theta}_n - \tilde{\theta} \big|
    \geq \epsilon \Big)
    \geq \delta.
  \end{equation*}
  But this is a contradiction to the fact that
  $\Psi(\hat{\theta}_n) \stackrel{p}{\longrightarrow} \Psi(\tilde{\theta})$.
\end{proof}

So it is established that the classification rate of the empirically optimal
quantile classifier converges to the classification rate of the true optimal
quantile classifier.  Next, we investigate the practical considerations of
calculating the empirically optimal quantile classifier.

\begin{enumerate}[label=\arabic*., align=left]
\item sort $v_i \in \Pi_V$ and $w_i \in \Pi_W$
\item calculate the set of quantile levels given by
  \begin{equation*}
    \Theta =
    \left\{ \frac{k}{n_0}: 1 \leq k \leq n_0 \} \right\} \bigcup
    \left\{ \frac{k}{n_1}: 1 \leq k \leq n_1 \} \right\}
  \end{equation*}
\item for $i$ in 1 to $n - 1$; do
  \begin{enumerate}[label=\alph*)]
  \item calculate $\hat{F}_V^{-1}(\theta_i)$ and $\hat{F}_W^{-1}(\theta_i)$, and
    find
    \begin{equation*}
      a = \min \Big\{ \hat{F}_V^{-1}(\theta_i),~ \hat{F}_W^{-1}(\theta_i) \Big\},
      \hspace{5mm} \text{and} \hspace{5mm}
      b = \max \Big\{ \hat{F}_V^{-1}(\theta_i),~ \hat{F}_W^{-1}(\theta_i) \Big\}
    \end{equation*}
  \item calculate the interval
    $G_i = \Big(
    \theta_{i+1}a + (1 - \theta_{i+1}) b,
    \hspace{3mm}
    \theta_{i}a + (1 - \theta_{i})\, b\,
    \Big]$
  \end{enumerate}
\item calculate $H_1, \dots, H_r$ the collection of disjoint intervals formed by
  joining overlapping intervals of $G_i$
\item for $H_k$ in $H_1, \dots, H_r$; do
  \begin{enumerate}[label=\alph*)]
  \item find the largest values of $v_i$ and $w_j$ smaller then every element in
    $H_k$
  \item calculate the classification rate for the decision rule at $\inf H_k$
  \item while $v_{i+i}$ and $w_{j+1}$ are not both greater than every element in
    $H_k$
    \begin{enumerate}[label=\roman*.]
    \item if $v_{i+1} \leq w_{j+1}$ then set $i = i + 1$; else set $j = j + 1$
    \item calculate the classification rule for the decision rule at the newly
      updated value of $v_i$ or $w_j$
    \end{enumerate}
  \end{enumerate}
\end{enumerate}

\begin{lemma}
  The decision rule for the empirically optimal quantile classifier can be
  obtained in $\mathcal{O}(n \log n)$ time.
\end{lemma}

\begin{proof}
  Sorting the data is an $\mathcal{O}(n \log n)$ operation.  Steps 2-4 take
  $\mathcal{O}(n)$ time.  Step 5 is an $\mathcal{O}(n \log n)$ operation.  For
  step 5, essentially for each iteration finding the initial values of $v_i$ and
  $w_j$ are a $\mathcal{O}(\log n)$ operation, and the sum of the operations in
  step 5c is an $\mathcal{O}(n)$ operation.
\end{proof}

So the empirically optimal quantile classifier can be efficiently calculated.
In practice, we might be willing to approximate the empirically optimal quantile
classifier by simply calculating the empirical classification rate over a grid
of quantile levels.  This reduces the complexity to an $\mathcal{O}(Kn)$
operation, where $K$ is the number of points in the grid.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "cqc_paper"
%%% End:
