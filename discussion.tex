
\section{Discussion}
\label{sec:discussion}

We propose a family of classification rules based on the marginal quantiles of
the class features.  The univariate quantile classifier is equal to the Bayes
rule for the optimal choice of quantile level and under some assumptions, so we
consider this as a starting point for constructing a multivariate classifier.
The multivariate classifiers considered in this paper are constructed in a two
step process.  First, the marginal quantiles of the data are estimated and an
estimate of the optimal quantile level for each component is calculated.
Secondly, a rule for combining the information provided by an observation's
quantile distances to the marginal within-class quantile is constructed through
a linear combination obtained using penalized linear regression.  We observe
competitive performance of composite quantile-based classifiers in simulation
examples and a spam email classification application.  The composite
quantile-based classifiers decision rule is computationally efficient to train
and the decision rule boundary has a simple piecewise-linear form that is
well-suited for high-dimensional settings.



% We observe competitive performance from the proposed classifier when the sample
% size is large enough to accurately estimate the class quantiles.  When sample
% size is relatively small then the classifier is unstable due to the many moving
% parts, so that other classification methods may be more appropriate.  But for
% moderate sample sizes, the proposed classifier appears to exhibit competitive
% performance in a variety of settings and provides an alternative methodology for
% the classification problem.




% \subsection{Potential instabilities and limitiations}
% \label{sec:instabilities-and-limitations}

% \subsubsection{Potential instabilities of composite quantile-based methods}
% \label{sec:instabilities}

% One potential source of instability of composite quantile-based classifiers is
% that the direction of the decision rule boundary lines are dependent on the
% magnitude of the difference between the component-wise class quantiles.  This
% raises two concerns.  Firstly, changes in scale of the features can
% fundamentally affect the classifier by changing which components are part of the
% linear systems in the piecewise affine sets.  As a result, when some of the
% features in the data are very different in scale to other features then we
% recommend some sort of data transformations to try to keep the features at a
% similar scale before performing classification.

% On the other hand, a second potential source of instability for composite
% quantile-based classifiers occurs when the magnitude of the differences between
% the within-class quantiles is very similar for two or more features.  When this
% is the case, small changes in the within-class quantile estimates can
% fundamentally affect the classifier by changing which components are part of the
% linear systems in the piecewise affine sets.  On the other hand, when the
% within-class quantiles are similar for a given component, this has the effect
% that the component contributes very little information to the classifier
% aggregate, so that although the form of the classifier may show some instability
% the classification rate will typically remain unaffected.  We also expect
% averaging the model as discussed in Section \ref{sec:classifier-algorithm} to
% help protect against this type of instability.


% \subsubsection{Limitations of composite quantile-based classifiers}
% \label{sec:limitations}

% There are two main limitations of composite quantile-based classifiers that are
% evident based upon the form of the classifiers.  The first limitation is that
% the classifiers decision rule boundaries are piecewise linear and consequently
% can only hope to approximate nonlinear decision rule boundaries.  For some
% problems this can result in a loss of efficiency when compared to more flexible
% methods.

% A second limitation of composite quantile-based classifiers is that there can
% only be a single decision rule boundary for any dimension.  Consider an example
% where the densities of two populations are perfectly separated into two
% concentric circles.  The Bayes rule can perfectly classify this situation, but
% any composite quantile-based classifiers decision rule boundary is totally
% unable to capture both sides of the inner circle.

% We view the limitations of composite quantile-based classifiers as a trade-off
% between simplicity of the classifier and its ability to perform well in all
% situations.  No single classifier can be expected to perform well in all
% settings.  Due to the simple nature of the classifier it is well-suited for
% high-dimensional problems, even in the presence of limited data.  However, it is
% important to recognize that there are settings for which composite
% quantile-based classifiers are ill-suited.





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "cqc_paper"
%%% End:
