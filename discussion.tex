
\section{Discussion}
\label{sec:discussion}

We propose a family of classification rules based on the marginal quantiles of
the class features.  The quantile classifier is equal to the Bayes rule for the
ideal choice of quantile level, so building a classification rule from this
starting point seems like a promising endeavor.  Each of the classifiers
considered in this paper are constructed in a two step process.  First, the
marginal quantiles of the data are estimated and an estimate of the optimal
quantile level for each component is calculated.  Secondly, a rule for combining
the information provided by a point's quantile distance to the marginal class
quantile levels is constructed.  We consider simple summation of the
component-wise quantile distances, but there are some issues that cause problems
with performance.  Instead, we suggest combining the component-wise quantile
distances using a linear combination obtained through a regularization.

We observe competitive performance from the proposed classifier when the sample
size is large enough to accurately estimate the class quantiles.  When sample
size is relatively small then the classifier is unstable due to the many moving
parts, so that other classification methods may be more appropriate.  But for
moderate sample sizes, the proposed classifier appears to exhibit competitive
performance in a variety of settings and provides an alternative methodology for
the classification problem.




\subsection{Potential instabilities and limitiations}
\label{sec:instabilities-and-limitations}

\subsubsection{Potential instabilities of composite quantile-based methods}
\label{sec:instabilities}

One potential source of instability of composite quantile-based classifiers is
that the direction of the decision rule boundary lines are dependent on the
magnitude of the difference between the component-wise class quantiles.  This
raises two concerns.  Firstly, changes in scale of the features can
fundamentally affect the classifier by changing which components are part of the
linear systems in the piecewise affine sets.  As a result, when some of the
features in the data are very different in scale to other features then we
recommend some sort of data transformations to try to keep the features at a
similar scale before performing classification.

On the other hand, a second potential source of instability for composite
quantile-based classifiers occurs when the magnitude of the differences between
the within-class quantiles is very similar for two or more features.  When this
is the case, small changes in the within-class quantile estimates can
fundamentally affect the classifier by changing which components are part of the
linear systems in the piecewise affine sets.  On the other hand, when the
within-class quantiles are similar for a given component, this has the effect
that the component contributes very little information to the classifier
aggregate, so that although the form of the classifier may show some instability
the classification rate will typically remain unaffected.  We also expect
averaging the model as discussed in Section \ref{sec:classifier-algorithm} to
help protect against this type of instability.


\subsubsection{Limitations of composite quantile-based classifiers}
\label{sec:limitations}

There are two main limitations of composite quantile-based classifiers that are
evident based upon the form of the classifiers.  The first limitation is that
the classifiers decision rule boundaries are piecewise linear and consequently
can only hope to approximate nonlinear decision rule boundaries.  For some
problems this can result in a loss of efficiency when compared to more flexible
methods.

A second limitation of composite quantile-based classifiers is that there can
only be a single decision rule boundary for any dimension.  Consider an example
where the densities of two populations are perfectly separated into two
concentric circles.  The Bayes rule can perfectly classify this situation, but
any composite quantile-based classifiers decision rule boundary is totally
unable to capture both sides of the inner circle.

We view the limitations of composite quantile-based classifiers as a trade-off
between simplicity of the classifier and its ability to perform well in all
situations.  No single classifier can be expected to perform well in all
settings.  Due to the simple nature of the classifier it is well-suited for
high-dimensional problems, even in the presence of limited data.  However, it is
important to recognize that there are settings for which composite
quantile-based classifiers are ill-suited.





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "mqc_paper"
%%% End:
